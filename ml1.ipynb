{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.cross_validation\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setPred(col):\n",
    "    if(col > 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparar/tweks datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(\"features0,0.csv\")\n",
    "prediccion = pd.read_csv(\"csv/labels_training_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mergeo features y la info q nos tiran, lleno con 0 los q no se\n",
    "features = pd.merge(prediccion,features,on=\"person\",how= \"left\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.drop([\"person\",\"Unnamed: 0\",\"label\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.drop([\"level_0\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in features.columns:\n",
    "    features[word] = pd.to_numeric(features[word],errors = \"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, prediccion, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "# historial cambios tweaks | score\n",
    "\n",
    "columnas: \n",
    "'ad campaign hit', 'brand listing', 'checkout', 'generic listing',\n",
    "       'lead', 'search engine hit', 'searched products', 'staticpage',\n",
    "       'viewed product', 'visited site', 'level_0', 'storage_no_convercion',\n",
    "       'numero mas usado dia', 'condition_no_convercion2',\n",
    "       'color_no_convercion2', 'search_engine2', 'nombre mas usado dia2']\n",
    "\n",
    "test_size = 0.3\n",
    "cv = 20\n",
    "n_estimator 50 | .78 (Kaggle .76)\n",
    "n_estimator 100 | .79 \n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=BSUMBBFjxrY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'criterion': 'mse',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': -1,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomforesttree = RandomForestRegressor(random_state = None, n_jobs=-1)\n",
    "randomforesttree.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "JoblibException",
     "evalue": "JoblibException\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python3.5/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    179         sys.exit(msg)\n    180     main_globals = sys.modules[\"__main__\"].__dict__\n    181     if alter_argv:\n    182         sys.argv[0] = mod_spec.origin\n    183     return _run_code(code, main_globals, None,\n--> 184                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py')\n    185 \n    186 def run_module(mod_name, init_globals=None,\n    187                run_name=None, alter_sys=False):\n    188     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python3.5/runpy.py in _run_code(code=<code object <module> at 0x7f4e7a34cc90, file \"/...3.5/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/tito/.local/lib/python3.5/site-packages/__pycache__/ipykernel_launcher.cpython-35.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/tito/..../python3.5/site-packages/ipykernel/kernelapp.py'>, 'sys': <module 'sys' (built-in)>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f4e7a34cc90, file \"/...3.5/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/tito/.local/lib/python3.5/site-packages/__pycache__/ipykernel_launcher.cpython-35.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/tito/..../python3.5/site-packages/ipykernel/kernelapp.py'>, 'sys': <module 'sys' (built-in)>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n     17 \n     18 \n     19 \n     20 \n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    481         if self.poller is not None:\n    482             self.poller.start()\n    483         self.kernel.start()\n    484         self.io_loop = ioloop.IOLoop.current()\n    485         try:\n--> 486             self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    487         except KeyboardInterrupt:\n    488             pass\n    489 \n    490 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    107         except RuntimeError:\n    108             old_loop = None\n    109         try:\n    110             self._setup_logging()\n    111             asyncio.set_event_loop(self.asyncio_loop)\n--> 112             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Uni...EventLoop running=True closed=False debug=False>>\n    113         finally:\n    114             asyncio.set_event_loop(old_loop)\n    115 \n    116     def stop(self):\n\n...........................................................................\n/usr/lib/python3.5/asyncio/base_events.py in run_forever(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n    340             raise RuntimeError('Event loop is running.')\n    341         self._set_coroutine_wrapper(self._debug)\n    342         self._thread_id = threading.get_ident()\n    343         try:\n    344             while True:\n--> 345                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_UnixS...EventLoop running=True closed=False debug=False>>\n    346                 if self._stopping:\n    347                     break\n    348         finally:\n    349             self._stopping = False\n\n...........................................................................\n/usr/lib/python3.5/asyncio/base_events.py in _run_once(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n   1307                         logger.warning('Executing %s took %.3f seconds',\n   1308                                        _format_handle(handle), dt)\n   1309                 finally:\n   1310                     self._current_handle = None\n   1311             else:\n-> 1312                 handle._run()\n        handle._run = <bound method Handle._run of <Handle BaseAsyncIOLoop._handle_events(13, 1)>>\n   1313         handle = None  # Needed to break cycles when an exception occurs.\n   1314 \n   1315     def _set_coroutine_wrapper(self, enabled):\n   1316         try:\n\n...........................................................................\n/usr/lib/python3.5/asyncio/events.py in _run(self=<Handle BaseAsyncIOLoop._handle_events(13, 1)>)\n    120             self._callback = None\n    121             self._args = None\n    122 \n    123     def _run(self):\n    124         try:\n--> 125             self._callback(*self._args)\n        self._callback = <bound method BaseAsyncIOLoop._handle_events of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n        self._args = (13, 1)\n    126         except Exception as exc:\n    127             cb = _format_callback_source(self._callback, self._args)\n    128             msg = 'Exception in callback {}'.format(cb)\n    129             context = {\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py in _handle_events(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, fd=13, events=1)\n     97             self.writers.remove(fd)\n     98         del self.handlers[fd]\n     99 \n    100     def _handle_events(self, fd, events):\n    101         fileobj, handler_func = self.handlers[fd]\n--> 102         handler_func(fileobj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fileobj = <zmq.sugar.socket.Socket object>\n        events = 1\n    103 \n    104     def start(self):\n    105         try:\n    106             old_loop = asyncio.get_event_loop()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    445             return\n    446         zmq_events = self.socket.EVENTS\n    447         try:\n    448             # dispatch events:\n    449             if zmq_events & zmq.POLLIN and self.receiving():\n--> 450                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    451                 if not self.socket:\n    452                     return\n    453             if zmq_events & zmq.POLLOUT and self.sending():\n    454                 self._handle_send()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    475             else:\n    476                 raise\n    477         else:\n    478             if self._recv_callback:\n    479                 callback = self._recv_callback\n--> 480                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    481         \n    482 \n    483     def _handle_send(self):\n    484         \"\"\"Handle a send event.\"\"\"\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    427         close our socket.\"\"\"\n    428         try:\n    429             # Use a NullContext to ensure that all StackContexts are run\n    430             # inside our blanket exception handler rather than outside.\n    431             with stack_context.NullContext():\n--> 432                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    433         except:\n    434             gen_log.error(\"Uncaught exception in ZMQStream callback\",\n    435                           exc_info=True)\n    436             # Re-raise the exception so that IOLoop.handle_callback_exception\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'43d8b9a1122d4f57be7c23f396e940a0']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'43d8b9a1122d4f57be7c23f396e940a0'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>], cell_name='<ipython-input-462-88a493e9dcad>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>)\n   2845 \n   2846         try:\n   2847             for i, node in enumerate(to_run_exec):\n   2848                 mod = ast.Module([node])\n   2849                 code = compiler(mod, cell_name, \"exec\")\n-> 2850                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>\n        result = <ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>, result=<ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>\n        self.user_global_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...m sklearn.cross_validation import cross_val_score', 'features = pd.read_csv(\"features0,0.csv\")', 'randomforesttree = RandomForestClassifier(random...=103040, n_jobs=-1)\\nrandomforesttree.get_params()', 'features.columns', 'def setPred(col):\\n    if(col != 0):\\n        return 1\\n    else:\\n        return 0', '\\n\\nfeatures[\"prediction\"] = setPred(features[\"conversion\"])', 'features = pd.read_csv(\"features0,0.csv\")', 'setPred(features[\"conversion\"])', 'setPred(features[\"conversion\"].values)', 'features[\"conversion\"].apply(setPred)', 'features[\"conversion\"].apply(setPred).sum()', 'features[\"conversion\"]', 'features = pd.read_csv(\"features0,0.csv\")', 'features[\"prediction\"] = features[\"conversion\"]', 'features[\"prediction\"].mean()', 'features[\"prediction\"].max()', 'features[\"prediction\"] = features[\"conversion\"].applu(setPred)', 'features[\"prediction\"] = features[\"conversion\"].apply(setPred)', 'features[\"prediction\"].max()', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {3: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, 4: Index(['Unnamed: 0', 'ad campaign hit', 'brand l...   'nombre mas usado dia'],\n      dtype='object'), 10: 0       1\n1       1\n2       1\n3       1\n4       ...   1\nName: conversion, Length: 1712, dtype: int64, 11: 1712, 12: 0       2.0\n1       1.0\n2       1.0\n3       1.0\n....0\nName: conversion, Length: 1712, dtype: float64, 15: 0.1853565453785027, 16: 129.0, 19: 1, 21: 0.11221769134253451, 22: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, ...}\n        self.user_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...m sklearn.cross_validation import cross_val_score', 'features = pd.read_csv(\"features0,0.csv\")', 'randomforesttree = RandomForestClassifier(random...=103040, n_jobs=-1)\\nrandomforesttree.get_params()', 'features.columns', 'def setPred(col):\\n    if(col != 0):\\n        return 1\\n    else:\\n        return 0', '\\n\\nfeatures[\"prediction\"] = setPred(features[\"conversion\"])', 'features = pd.read_csv(\"features0,0.csv\")', 'setPred(features[\"conversion\"])', 'setPred(features[\"conversion\"].values)', 'features[\"conversion\"].apply(setPred)', 'features[\"conversion\"].apply(setPred).sum()', 'features[\"conversion\"]', 'features = pd.read_csv(\"features0,0.csv\")', 'features[\"prediction\"] = features[\"conversion\"]', 'features[\"prediction\"].mean()', 'features[\"prediction\"].max()', 'features[\"prediction\"] = features[\"conversion\"].applu(setPred)', 'features[\"prediction\"] = features[\"conversion\"].apply(setPred)', 'features[\"prediction\"].max()', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {3: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, 4: Index(['Unnamed: 0', 'ad campaign hit', 'brand l...   'nombre mas usado dia'],\n      dtype='object'), 10: 0       1\n1       1\n2       1\n3       1\n4       ...   1\nName: conversion, Length: 1712, dtype: int64, 11: 1712, 12: 0       2.0\n1       1.0\n2       1.0\n3       1.0\n....0\nName: conversion, Length: 1712, dtype: float64, 15: 0.1853565453785027, 16: 129.0, 19: 1, 21: 0.11221769134253451, 22: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\n/home/tito/Desktop/datas/gitNuevo/<ipython-input-462-88a493e9dcad> in <module>()\n      1 \n      2 \n----> 3 \n      4 param_grid= {'max_features': [\"auto\"], 'n_estimators': [100], 'criterion': ['mse',\"mae\"]}\n      5 grid_drop = GridSearchCV(randomforesttree, param_grid, cv=20, scoring='roc_auc',n_jobs = -1,pre_dispatch = 4)\n      6 grid_drop.fit(X_train, y_train)\n      7 print(grid_drop.best_params_, grid_drop.best_score_)\n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=20, error_score='raise',\n       ...atch=4, refit=True, scoring='roc_auc', verbose=0), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64)\n    799         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n    800             Target relative to X for classification or regression;\n    801             None for unsupervised learning.\n    802 \n    803         \"\"\"\n--> 804         return self._fit(X, y, ParameterGrid(self.param_grid))\n        self._fit = <bound method BaseSearchCV._fit of GridSearchCV(...tch=4, refit=True, scoring='roc_auc', verbose=0)>\n        X =        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns]\n        y = 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64\n        self.param_grid = {'criterion': ['mse', 'mae'], 'max_features': ['auto'], 'n_estimators': [100]}\n    805 \n    806 \n    807 class RandomizedSearchCV(BaseSearchCV):\n    808     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=20, error_score='raise',\n       ...atch=4, refit=True, scoring='roc_auc', verbose=0), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\n    548         )(\n    549             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    550                                     train, test, self.verbose, parameters,\n    551                                     self.fit_params, return_parameters=True,\n    552                                     error_score=self.error_score)\n--> 553                 for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterGrid object>\n    554                 for train, test in cv)\n    555 \n    556         # Out is a list of triplet: score, estimator, n_test_samples\n    557         n_fits = len(out)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV._fit.<locals>.<genexpr>>)\n    805             if pre_dispatch == \"all\" or n_jobs == 1:\n    806                 # The iterable was consumed all at once by the above for loop.\n    807                 # No need to wait for async callbacks to trigger to\n    808                 # consumption.\n    809                 self._iterating = False\n--> 810             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    811             # Make sure that we get a last message telling us we are done\n    812             elapsed_time = time.time() - self._start_time\n    813             self._print('Done %3i out of %3i | elapsed: %s finished',\n    814                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nJoblibKeyError                                     Fri Nov  9 23:18:21 2018\nPID: 13242                                   Python 3.5.2: /usr/bin/python3\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {}), {'error_score': 'raise', 'return_parameters': True})]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/cross_validation.py in _fit_and_score(estimator=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, scorer=make_scorer(roc_auc_score, needs_threshold=True), train=array([  670,   671,   672, ..., 13386, 13387, 13388]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), verbose=0, parameters={'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1526 \n   1527     try:\n   1528         if y_train is None:\n   1529             estimator.fit(X_train, **fit_params)\n   1530         else:\n-> 1531             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestRegr...te=None,\n           verbose=0, warm_start=False)>\n        X_train =        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[12719 rows x 17 columns]\n        y_train = 6507     0\n12276    0\n4391     0\n18119    0\n9744...024    0\nName: label, Length: 12719, dtype: int64\n        fit_params = {}\n   1532 \n   1533     except Exception as e:\n   1534         if error_score == 'raise':\n   1535             raise\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py in fit(self=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=None)\n    285             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    286                              backend=\"threading\")(\n    287                 delayed(_parallel_build_trees)(\n    288                     t, self, X, y, sample_weight, i, len(trees),\n    289                     verbose=self.verbose, class_weight=self.class_weight)\n--> 290                 for i, t in enumerate(trees))\n        i = 99\n    291 \n    292             # Collect newly grown trees\n    293             self.estimators_.extend(trees)\n    294 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    805             if pre_dispatch == \"all\" or n_jobs == 1:\n    806                 # The iterable was consumed all at once by the above for loop.\n    807                 # No need to wait for async callbacks to trigger to\n    808                 # consumption.\n    809                 self._iterating = False\n--> 810             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    811             # Make sure that we get a last message telling us we are done\n    812             elapsed_time = time.time() - self._start_time\n    813             self._print('Done %3i out of %3i | elapsed: %s finished',\n    814                         (len(self._output), len(self._output),\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in retrieve(self=Parallel(n_jobs=-1))\n    752                     # In case we had to terminate a managed pool, let\n    753                     # us start a new one to ensure that subsequent calls\n    754                     # to __call__ on the same Parallel instance will get\n    755                     # a working pool as they expect.\n    756                     self._initialize_pool()\n--> 757                 raise exception\n        exception = undefined\n    758 \n    759     def __call__(self, iterable):\n    760         if self._jobs:\n    761             raise ValueError('This Parallel instance is already running')\n\nJoblibKeyError: JoblibKeyError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python3.5/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    179         sys.exit(msg)\n    180     main_globals = sys.modules[\"__main__\"].__dict__\n    181     if alter_argv:\n    182         sys.argv[0] = mod_spec.origin\n    183     return _run_code(code, main_globals, None,\n--> 184                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py')\n    185 \n    186 def run_module(mod_name, init_globals=None,\n    187                run_name=None, alter_sys=False):\n    188     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python3.5/runpy.py in _run_code(code=<code object <module> at 0x7f4e7a34cc90, file \"/...3.5/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/tito/.local/lib/python3.5/site-packages/__pycache__/ipykernel_launcher.cpython-35.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/tito/..../python3.5/site-packages/ipykernel/kernelapp.py'>, 'sys': <module 'sys' (built-in)>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f4e7a34cc90, file \"/...3.5/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/tito/.local/lib/python3.5/site-packages/__pycache__/ipykernel_launcher.cpython-35.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/tito/..../python3.5/site-packages/ipykernel/kernelapp.py'>, 'sys': <module 'sys' (built-in)>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n     17 \n     18 \n     19 \n     20 \n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    481         if self.poller is not None:\n    482             self.poller.start()\n    483         self.kernel.start()\n    484         self.io_loop = ioloop.IOLoop.current()\n    485         try:\n--> 486             self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    487         except KeyboardInterrupt:\n    488             pass\n    489 \n    490 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    107         except RuntimeError:\n    108             old_loop = None\n    109         try:\n    110             self._setup_logging()\n    111             asyncio.set_event_loop(self.asyncio_loop)\n--> 112             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Uni...EventLoop running=True closed=False debug=False>>\n    113         finally:\n    114             asyncio.set_event_loop(old_loop)\n    115 \n    116     def stop(self):\n\n...........................................................................\n/usr/lib/python3.5/asyncio/base_events.py in run_forever(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n    340             raise RuntimeError('Event loop is running.')\n    341         self._set_coroutine_wrapper(self._debug)\n    342         self._thread_id = threading.get_ident()\n    343         try:\n    344             while True:\n--> 345                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_UnixS...EventLoop running=True closed=False debug=False>>\n    346                 if self._stopping:\n    347                     break\n    348         finally:\n    349             self._stopping = False\n\n...........................................................................\n/usr/lib/python3.5/asyncio/base_events.py in _run_once(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n   1307                         logger.warning('Executing %s took %.3f seconds',\n   1308                                        _format_handle(handle), dt)\n   1309                 finally:\n   1310                     self._current_handle = None\n   1311             else:\n-> 1312                 handle._run()\n        handle._run = <bound method Handle._run of <Handle BaseAsyncIOLoop._handle_events(13, 1)>>\n   1313         handle = None  # Needed to break cycles when an exception occurs.\n   1314 \n   1315     def _set_coroutine_wrapper(self, enabled):\n   1316         try:\n\n...........................................................................\n/usr/lib/python3.5/asyncio/events.py in _run(self=<Handle BaseAsyncIOLoop._handle_events(13, 1)>)\n    120             self._callback = None\n    121             self._args = None\n    122 \n    123     def _run(self):\n    124         try:\n--> 125             self._callback(*self._args)\n        self._callback = <bound method BaseAsyncIOLoop._handle_events of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n        self._args = (13, 1)\n    126         except Exception as exc:\n    127             cb = _format_callback_source(self._callback, self._args)\n    128             msg = 'Exception in callback {}'.format(cb)\n    129             context = {\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py in _handle_events(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, fd=13, events=1)\n     97             self.writers.remove(fd)\n     98         del self.handlers[fd]\n     99 \n    100     def _handle_events(self, fd, events):\n    101         fileobj, handler_func = self.handlers[fd]\n--> 102         handler_func(fileobj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fileobj = <zmq.sugar.socket.Socket object>\n        events = 1\n    103 \n    104     def start(self):\n    105         try:\n    106             old_loop = asyncio.get_event_loop()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    445             return\n    446         zmq_events = self.socket.EVENTS\n    447         try:\n    448             # dispatch events:\n    449             if zmq_events & zmq.POLLIN and self.receiving():\n--> 450                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    451                 if not self.socket:\n    452                     return\n    453             if zmq_events & zmq.POLLOUT and self.sending():\n    454                 self._handle_send()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    475             else:\n    476                 raise\n    477         else:\n    478             if self._recv_callback:\n    479                 callback = self._recv_callback\n--> 480                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    481         \n    482 \n    483     def _handle_send(self):\n    484         \"\"\"Handle a send event.\"\"\"\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    427         close our socket.\"\"\"\n    428         try:\n    429             # Use a NullContext to ensure that all StackContexts are run\n    430             # inside our blanket exception handler rather than outside.\n    431             with stack_context.NullContext():\n--> 432                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    433         except:\n    434             gen_log.error(\"Uncaught exception in ZMQStream callback\",\n    435                           exc_info=True)\n    436             # Re-raise the exception so that IOLoop.handle_callback_exception\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'43d8b9a1122d4f57be7c23f396e940a0']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'43d8b9a1122d4f57be7c23f396e940a0'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>], cell_name='<ipython-input-462-88a493e9dcad>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>)\n   2845 \n   2846         try:\n   2847             for i, node in enumerate(to_run_exec):\n   2848                 mod = ast.Module([node])\n   2849                 code = compiler(mod, cell_name, \"exec\")\n-> 2850                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>\n        result = <ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>, result=<ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>\n        self.user_global_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...m sklearn.cross_validation import cross_val_score', 'features = pd.read_csv(\"features0,0.csv\")', 'randomforesttree = RandomForestClassifier(random...=103040, n_jobs=-1)\\nrandomforesttree.get_params()', 'features.columns', 'def setPred(col):\\n    if(col != 0):\\n        return 1\\n    else:\\n        return 0', '\\n\\nfeatures[\"prediction\"] = setPred(features[\"conversion\"])', 'features = pd.read_csv(\"features0,0.csv\")', 'setPred(features[\"conversion\"])', 'setPred(features[\"conversion\"].values)', 'features[\"conversion\"].apply(setPred)', 'features[\"conversion\"].apply(setPred).sum()', 'features[\"conversion\"]', 'features = pd.read_csv(\"features0,0.csv\")', 'features[\"prediction\"] = features[\"conversion\"]', 'features[\"prediction\"].mean()', 'features[\"prediction\"].max()', 'features[\"prediction\"] = features[\"conversion\"].applu(setPred)', 'features[\"prediction\"] = features[\"conversion\"].apply(setPred)', 'features[\"prediction\"].max()', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {3: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, 4: Index(['Unnamed: 0', 'ad campaign hit', 'brand l...   'nombre mas usado dia'],\n      dtype='object'), 10: 0       1\n1       1\n2       1\n3       1\n4       ...   1\nName: conversion, Length: 1712, dtype: int64, 11: 1712, 12: 0       2.0\n1       1.0\n2       1.0\n3       1.0\n....0\nName: conversion, Length: 1712, dtype: float64, 15: 0.1853565453785027, 16: 129.0, 19: 1, 21: 0.11221769134253451, 22: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, ...}\n        self.user_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...m sklearn.cross_validation import cross_val_score', 'features = pd.read_csv(\"features0,0.csv\")', 'randomforesttree = RandomForestClassifier(random...=103040, n_jobs=-1)\\nrandomforesttree.get_params()', 'features.columns', 'def setPred(col):\\n    if(col != 0):\\n        return 1\\n    else:\\n        return 0', '\\n\\nfeatures[\"prediction\"] = setPred(features[\"conversion\"])', 'features = pd.read_csv(\"features0,0.csv\")', 'setPred(features[\"conversion\"])', 'setPred(features[\"conversion\"].values)', 'features[\"conversion\"].apply(setPred)', 'features[\"conversion\"].apply(setPred).sum()', 'features[\"conversion\"]', 'features = pd.read_csv(\"features0,0.csv\")', 'features[\"prediction\"] = features[\"conversion\"]', 'features[\"prediction\"].mean()', 'features[\"prediction\"].max()', 'features[\"prediction\"] = features[\"conversion\"].applu(setPred)', 'features[\"prediction\"] = features[\"conversion\"].apply(setPred)', 'features[\"prediction\"].max()', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {3: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, 4: Index(['Unnamed: 0', 'ad campaign hit', 'brand l...   'nombre mas usado dia'],\n      dtype='object'), 10: 0       1\n1       1\n2       1\n3       1\n4       ...   1\nName: conversion, Length: 1712, dtype: int64, 11: 1712, 12: 0       2.0\n1       1.0\n2       1.0\n3       1.0\n....0\nName: conversion, Length: 1712, dtype: float64, 15: 0.1853565453785027, 16: 129.0, 19: 1, 21: 0.11221769134253451, 22: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\n/home/tito/Desktop/datas/gitNuevo/<ipython-input-462-88a493e9dcad> in <module>()\n      1 \n      2 \n----> 3 \n      4 param_grid= {'max_features': [\"auto\"], 'n_estimators': [100], 'criterion': ['mse',\"mae\"]}\n      5 grid_drop = GridSearchCV(randomforesttree, param_grid, cv=20, scoring='roc_auc',n_jobs = -1,pre_dispatch = 4)\n      6 grid_drop.fit(X_train, y_train)\n      7 print(grid_drop.best_params_, grid_drop.best_score_)\n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=20, error_score='raise',\n       ...atch=4, refit=True, scoring='roc_auc', verbose=0), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64)\n    799         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n    800             Target relative to X for classification or regression;\n    801             None for unsupervised learning.\n    802 \n    803         \"\"\"\n--> 804         return self._fit(X, y, ParameterGrid(self.param_grid))\n        self._fit = <bound method BaseSearchCV._fit of GridSearchCV(...tch=4, refit=True, scoring='roc_auc', verbose=0)>\n        X =        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns]\n        y = 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64\n        self.param_grid = {'criterion': ['mse', 'mae'], 'max_features': ['auto'], 'n_estimators': [100]}\n    805 \n    806 \n    807 class RandomizedSearchCV(BaseSearchCV):\n    808     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=20, error_score='raise',\n       ...atch=4, refit=True, scoring='roc_auc', verbose=0), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\n    548         )(\n    549             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    550                                     train, test, self.verbose, parameters,\n    551                                     self.fit_params, return_parameters=True,\n    552                                     error_score=self.error_score)\n--> 553                 for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterGrid object>\n    554                 for train, test in cv)\n    555 \n    556         # Out is a list of triplet: score, estimator, n_test_samples\n    557         n_fits = len(out)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV._fit.<locals>.<genexpr>>)\n    761             raise ValueError('This Parallel instance is already running')\n    762         # A flag used to abort the dispatching of jobs in case an\n    763         # exception is found\n    764         self._aborting = False\n    765         if not self._managed_pool:\n--> 766             n_jobs = self._initialize_pool()\n        n_jobs = undefined\n        self._initialize_pool = <bound method Parallel._initialize_pool of Parallel(n_jobs=-1)>\n    767         else:\n    768             n_jobs = self._effective_n_jobs()\n    769 \n    770         if self.batch_size == 'auto':\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in _initialize_pool(self=Parallel(n_jobs=-1))\n    532                     verbose=max(0, self.verbose - 50),\n    533                 )\n    534                 if self._mp_context is not None:\n    535                     # Use Python 3.4+ multiprocessing context isolation\n    536                     poolargs['context'] = self._mp_context\n--> 537                 self._pool = MemmapingPool(n_jobs, **poolargs)\n        self._pool = None\n        n_jobs = 4\n        poolargs = {'context': <multiprocessing.context.ForkContext object>, 'max_nbytes': 1048576, 'mmap_mode': 'r', 'temp_folder': None, 'verbose': 0}\n    538 \n    539                 # We are using multiprocessing, we also want to capture\n    540                 # KeyboardInterrupts\n    541                 self.exceptions.extend([KeyboardInterrupt, WorkerInterrupt])\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/pool.py in __init__(self=<joblib.pool.MemmapingPool object>, processes=4, temp_folder='/dev/shm', max_nbytes=1048576, mmap_mode='r', forward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, backward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, verbose=0, context_id=None, prewarm=False, **kwargs={'context': <multiprocessing.context.ForkContext object>})\n    575         poolargs = dict(\n    576             processes=processes,\n    577             forward_reducers=forward_reducers,\n    578             backward_reducers=backward_reducers)\n    579         poolargs.update(kwargs)\n--> 580         super(MemmapingPool, self).__init__(**poolargs)\n        self.__init__ = <bound method MemmapingPool.__init__ of <joblib.pool.MemmapingPool object>>\n        poolargs = {'backward_reducers': {<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, 'context': <multiprocessing.context.ForkContext object>, 'forward_reducers': {<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, 'processes': 4}\n    581 \n    582     def terminate(self):\n    583         super(MemmapingPool, self).terminate()\n    584         delete_folder(self._temp_folder)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/pool.py in __init__(self=<joblib.pool.MemmapingPool object>, processes=4, forward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, backward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, **kwargs={'context': <multiprocessing.context.ForkContext object>})\n    413             backward_reducers = dict()\n    414         self._forward_reducers = forward_reducers\n    415         self._backward_reducers = backward_reducers\n    416         poolargs = dict(processes=processes)\n    417         poolargs.update(kwargs)\n--> 418         super(PicklingPool, self).__init__(**poolargs)\n        self.__init__ = <bound method MemmapingPool.__init__ of <joblib.pool.MemmapingPool object>>\n        poolargs = {'context': <multiprocessing.context.ForkContext object>, 'processes': 4}\n    419 \n    420     def _setup_queues(self):\n    421         context = getattr(self, '_ctx', mp)\n    422         self._inqueue = CustomizablePicklingQueue(context,\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/pool.py in __init__(self=<joblib.pool.MemmapingPool object>, processes=4, initializer=None, initargs=(), maxtasksperchild=None, context=<multiprocessing.context.ForkContext object>)\n    163         if initializer is not None and not callable(initializer):\n    164             raise TypeError('initializer must be a callable')\n    165 \n    166         self._processes = processes\n    167         self._pool = []\n--> 168         self._repopulate_pool()\n        self._repopulate_pool = <bound method Pool._repopulate_pool of <joblib.pool.MemmapingPool object>>\n    169 \n    170         self._worker_handler = threading.Thread(\n    171             target=Pool._handle_workers,\n    172             args=(self, )\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/pool.py in _repopulate_pool(self=<joblib.pool.MemmapingPool object>)\n    228                                    self._wrap_exception)\n    229                             )\n    230             self._pool.append(w)\n    231             w.name = w.name.replace('Process', 'PoolWorker')\n    232             w.daemon = True\n--> 233             w.start()\n        w.start = <bound method BaseProcess.start of <ForkProcess(ForkPoolWorker-130, started daemon)>>\n    234             util.debug('added worker')\n    235 \n    236     def _maintain_pool(self):\n    237         \"\"\"Clean up any exited workers and start replacements for them.\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/process.py in start(self=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n    100         assert self._parent_pid == os.getpid(), \\\n    101                'can only start a process object created by current process'\n    102         assert not _current_process._config.get('daemon'), \\\n    103                'daemonic processes are not allowed to have children'\n    104         _cleanup()\n--> 105         self._popen = self._Popen(self)\n        self._popen = None\n        self._Popen = <function ForkProcess._Popen>\n        self = <ForkProcess(ForkPoolWorker-130, started daemon)>\n    106         self._sentinel = self._popen.sentinel\n    107         _children.add(self)\n    108 \n    109     def terminate(self):\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/context.py in _Popen(process_obj=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n    262     class ForkProcess(process.BaseProcess):\n    263         _start_method = 'fork'\n    264         @staticmethod\n    265         def _Popen(process_obj):\n    266             from .popen_fork import Popen\n--> 267             return Popen(process_obj)\n        Popen = <class 'multiprocessing.popen_fork.Popen'>\n        process_obj = <ForkProcess(ForkPoolWorker-130, started daemon)>\n    268 \n    269     class SpawnProcess(process.BaseProcess):\n    270         _start_method = 'spawn'\n    271         @staticmethod\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/popen_fork.py in __init__(self=<multiprocessing.popen_fork.Popen object>, process_obj=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n     15 \n     16     def __init__(self, process_obj):\n     17         sys.stdout.flush()\n     18         sys.stderr.flush()\n     19         self.returncode = None\n---> 20         self._launch(process_obj)\n        self._launch = <bound method Popen._launch of <multiprocessing.popen_fork.Popen object>>\n        process_obj = <ForkProcess(ForkPoolWorker-130, started daemon)>\n     21 \n     22     def duplicate_for_child(self, fd):\n     23         return fd\n     24 \n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/popen_fork.py in _launch(self=<multiprocessing.popen_fork.Popen object>, process_obj=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n     69             try:\n     70                 os.close(parent_r)\n     71                 if 'random' in sys.modules:\n     72                     import random\n     73                     random.seed()\n---> 74                 code = process_obj._bootstrap()\n        code = 1\n        process_obj._bootstrap = <bound method BaseProcess._bootstrap of <ForkProcess(ForkPoolWorker-130, started daemon)>>\n     75             finally:\n     76                 os._exit(code)\n     77         else:\n     78             os.close(child_w)\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/process.py in _bootstrap(self=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n    244                 # delay finalization of the old process object until after\n    245                 # _run_after_forkers() is executed\n    246                 del old_process\n    247             util.info('child process calling self.run()')\n    248             try:\n--> 249                 self.run()\n        self.run = <bound method BaseProcess.run of <ForkProcess(ForkPoolWorker-130, started daemon)>>\n    250                 exitcode = 0\n    251             finally:\n    252                 util._exit_function()\n    253         except SystemExit as e:\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/process.py in run(self=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n     88     def run(self):\n     89         '''\n     90         Method to be run in sub-process; can be overridden in sub-class\n     91         '''\n     92         if self._target:\n---> 93             self._target(*self._args, **self._kwargs)\n        self._target = <function worker>\n        self._args = (<joblib.pool.CustomizablePicklingQueue object>, <joblib.pool.CustomizablePicklingQueue object>, None, (), None, True)\n        self._kwargs = {}\n     94 \n     95     def start(self):\n     96         '''\n     97         Start child process\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/pool.py in worker(inqueue=<joblib.pool.CustomizablePicklingQueue object>, outqueue=<joblib.pool.CustomizablePicklingQueue object>, initializer=None, initargs=(), maxtasks=None, wrap_exception=True)\n    114             util.debug('worker got sentinel -- exiting')\n    115             break\n    116 \n    117         job, i, func, args, kwds = task\n    118         try:\n--> 119             result = (True, func(*args, **kwds))\n        result = (True, [[0.7652468076996379, 669, 25.73760676383972, {'criterion': 'mse', 'max_features': 'auto', 'n_estimators': 100}]])\n        func = <joblib.parallel.SafeFunction object>\n        args = ()\n        kwds = {}\n    120         except Exception as e:\n    121             if wrap_exception:\n    122                 e = ExceptionWithTraceback(e, e.__traceback__)\n    123             result = (False, e)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.SafeFunction object>, *args=(), **kwargs={})\n    125     def __init__(self, func):\n    126         self.func = func\n    127 \n    128     def __call__(self, *args, **kwargs):\n    129         try:\n--> 130             return self.func(*args, **kwargs)\n        self.func = <joblib.parallel.BatchedCalls object>\n        args = ()\n        kwargs = {}\n    131         except KeyboardInterrupt:\n    132             # We capture the KeyboardInterrupt and reraise it as\n    133             # something different, as multiprocessing does not\n    134             # interrupt processing for a KeyboardInterrupt\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {}), {'error_score': 'raise', 'return_parameters': True})]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/cross_validation.py in _fit_and_score(estimator=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, scorer=make_scorer(roc_auc_score, needs_threshold=True), train=array([  670,   671,   672, ..., 13386, 13387, 13388]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), verbose=0, parameters={'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1526 \n   1527     try:\n   1528         if y_train is None:\n   1529             estimator.fit(X_train, **fit_params)\n   1530         else:\n-> 1531             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestRegr...te=None,\n           verbose=0, warm_start=False)>\n        X_train =        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[12719 rows x 17 columns]\n        y_train = 6507     0\n12276    0\n4391     0\n18119    0\n9744...024    0\nName: label, Length: 12719, dtype: int64\n        fit_params = {}\n   1532 \n   1533     except Exception as e:\n   1534         if error_score == 'raise':\n   1535             raise\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py in fit(self=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=None)\n    285             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    286                              backend=\"threading\")(\n    287                 delayed(_parallel_build_trees)(\n    288                     t, self, X, y, sample_weight, i, len(trees),\n    289                     verbose=self.verbose, class_weight=self.class_weight)\n--> 290                 for i, t in enumerate(trees))\n        i = 99\n    291 \n    292             # Collect newly grown trees\n    293             self.estimators_.extend(trees)\n    294 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    805             if pre_dispatch == \"all\" or n_jobs == 1:\n    806                 # The iterable was consumed all at once by the above for loop.\n    807                 # No need to wait for async callbacks to trigger to\n    808                 # consumption.\n    809                 self._iterating = False\n--> 810             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    811             # Make sure that we get a last message telling us we are done\n    812             elapsed_time = time.time() - self._start_time\n    813             self._print('Done %3i out of %3i | elapsed: %s finished',\n    814                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nKeyError                                           Fri Nov  9 23:18:19 2018\nPID: 13242                                   Python 3.5.2: /usr/bin/python3\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _parallel_build_trees>, (DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), None, 0, 100), {'class_weight': None, 'verbose': 0})]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _parallel_build_trees>\n        args = (DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), None, 0, 100)\n        kwargs = {'class_weight': None, 'verbose': 0}\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py in _parallel_build_trees(tree=DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), forest=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=None, tree_idx=0, n_trees=100, verbose=0, class_weight=None)\n    111                 warnings.simplefilter('ignore', DeprecationWarning)\n    112                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n    113         elif class_weight == 'balanced_subsample':\n    114             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n    115 \n--> 116         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n        tree.fit = <bound method BaseDecisionTree.fit of DecisionTr...         random_state=19780174, splitter='best')>\n        X = array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32)\n        y = array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]])\n        sample_weight = None\n        curr_sample_weight = array([1., 1., 0., ..., 0., 2., 1.])\n    117     else:\n    118         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    119 \n    120     return tree\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/tree/tree.py in fit(self=DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=array([1., 1., 0., ..., 0., 2., 1.]), check_input=False, X_idx_sorted=None)\n    317         if not isinstance(criterion, Criterion):\n    318             if is_classification:\n    319                 criterion = CRITERIA_CLF[self.criterion](self.n_outputs_,\n    320                                                          self.n_classes_)\n    321             else:\n--> 322                 criterion = CRITERIA_REG[self.criterion](self.n_outputs_)\n        criterion = 'mae'\n        self.criterion = 'mae'\n        self.n_outputs_ = 1\n    323 \n    324         SPLITTERS = SPARSE_SPLITTERS if issparse(X) else DENSE_SPLITTERS\n    325 \n    326         splitter = self.splitter\n\nKeyError: 'mae'\n___________________________________________________________________________\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3/dist-packages/joblib/parallel.py\", line 130, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/lib/python3/dist-packages/joblib/parallel.py\", line 72, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/usr/lib/python3/dist-packages/joblib/parallel.py\", line 72, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py\", line 116, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"/usr/lib/python3/dist-packages/sklearn/tree/tree.py\", line 322, in fit\n    criterion = CRITERIA_REG[self.criterion](self.n_outputs_)\nKeyError: 'mae'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3/dist-packages/joblib/parallel.py\", line 727, in retrieve\n    self._output.extend(job.get())\n  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 608, in get\n    raise self._value\n  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3/dist-packages/joblib/parallel.py\", line 140, in __call__\n    raise TransportableException(text, e_type)\njoblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nKeyError                                           Fri Nov  9 23:18:19 2018\nPID: 13242                                   Python 3.5.2: /usr/bin/python3\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _parallel_build_trees>, (DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), None, 0, 100), {'class_weight': None, 'verbose': 0})]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _parallel_build_trees>\n        args = (DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), None, 0, 100)\n        kwargs = {'class_weight': None, 'verbose': 0}\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py in _parallel_build_trees(tree=DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), forest=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=None, tree_idx=0, n_trees=100, verbose=0, class_weight=None)\n    111                 warnings.simplefilter('ignore', DeprecationWarning)\n    112                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n    113         elif class_weight == 'balanced_subsample':\n    114             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n    115 \n--> 116         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n        tree.fit = <bound method BaseDecisionTree.fit of DecisionTr...         random_state=19780174, splitter='best')>\n        X = array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32)\n        y = array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]])\n        sample_weight = None\n        curr_sample_weight = array([1., 1., 0., ..., 0., 2., 1.])\n    117     else:\n    118         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    119 \n    120     return tree\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/tree/tree.py in fit(self=DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=array([1., 1., 0., ..., 0., 2., 1.]), check_input=False, X_idx_sorted=None)\n    317         if not isinstance(criterion, Criterion):\n    318             if is_classification:\n    319                 criterion = CRITERIA_CLF[self.criterion](self.n_outputs_,\n    320                                                          self.n_classes_)\n    321             else:\n--> 322                 criterion = CRITERIA_REG[self.criterion](self.n_outputs_)\n        criterion = 'mae'\n        self.criterion = 'mae'\n        self.n_outputs_ = 1\n    323 \n    324         SPLITTERS = SPARSE_SPLITTERS if issparse(X) else DENSE_SPLITTERS\n    325 \n    326         splitter = self.splitter\n\nKeyError: 'mae'\n___________________________________________________________________________\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3/dist-packages/joblib/parallel.py\", line 130, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/lib/python3/dist-packages/joblib/parallel.py\", line 72, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/usr/lib/python3/dist-packages/joblib/parallel.py\", line 72, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/usr/lib/python3/dist-packages/sklearn/cross_validation.py\", line 1531, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py\", line 290, in fit\n    for i, t in enumerate(trees))\n  File \"/usr/lib/python3/dist-packages/joblib/parallel.py\", line 810, in __call__\n    self.retrieve()\n  File \"/usr/lib/python3/dist-packages/joblib/parallel.py\", line 757, in retrieve\n    raise exception\njoblib.my_exceptions.JoblibKeyError: JoblibKeyError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python3.5/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    179         sys.exit(msg)\n    180     main_globals = sys.modules[\"__main__\"].__dict__\n    181     if alter_argv:\n    182         sys.argv[0] = mod_spec.origin\n    183     return _run_code(code, main_globals, None,\n--> 184                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py')\n    185 \n    186 def run_module(mod_name, init_globals=None,\n    187                run_name=None, alter_sys=False):\n    188     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python3.5/runpy.py in _run_code(code=<code object <module> at 0x7f4e7a34cc90, file \"/...3.5/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/tito/.local/lib/python3.5/site-packages/__pycache__/ipykernel_launcher.cpython-35.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/tito/..../python3.5/site-packages/ipykernel/kernelapp.py'>, 'sys': <module 'sys' (built-in)>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f4e7a34cc90, file \"/...3.5/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/tito/.local/lib/python3.5/site-packages/__pycache__/ipykernel_launcher.cpython-35.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/tito/..../python3.5/site-packages/ipykernel/kernelapp.py'>, 'sys': <module 'sys' (built-in)>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n     17 \n     18 \n     19 \n     20 \n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    481         if self.poller is not None:\n    482             self.poller.start()\n    483         self.kernel.start()\n    484         self.io_loop = ioloop.IOLoop.current()\n    485         try:\n--> 486             self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    487         except KeyboardInterrupt:\n    488             pass\n    489 \n    490 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    107         except RuntimeError:\n    108             old_loop = None\n    109         try:\n    110             self._setup_logging()\n    111             asyncio.set_event_loop(self.asyncio_loop)\n--> 112             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Uni...EventLoop running=True closed=False debug=False>>\n    113         finally:\n    114             asyncio.set_event_loop(old_loop)\n    115 \n    116     def stop(self):\n\n...........................................................................\n/usr/lib/python3.5/asyncio/base_events.py in run_forever(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n    340             raise RuntimeError('Event loop is running.')\n    341         self._set_coroutine_wrapper(self._debug)\n    342         self._thread_id = threading.get_ident()\n    343         try:\n    344             while True:\n--> 345                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_UnixS...EventLoop running=True closed=False debug=False>>\n    346                 if self._stopping:\n    347                     break\n    348         finally:\n    349             self._stopping = False\n\n...........................................................................\n/usr/lib/python3.5/asyncio/base_events.py in _run_once(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n   1307                         logger.warning('Executing %s took %.3f seconds',\n   1308                                        _format_handle(handle), dt)\n   1309                 finally:\n   1310                     self._current_handle = None\n   1311             else:\n-> 1312                 handle._run()\n        handle._run = <bound method Handle._run of <Handle BaseAsyncIOLoop._handle_events(13, 1)>>\n   1313         handle = None  # Needed to break cycles when an exception occurs.\n   1314 \n   1315     def _set_coroutine_wrapper(self, enabled):\n   1316         try:\n\n...........................................................................\n/usr/lib/python3.5/asyncio/events.py in _run(self=<Handle BaseAsyncIOLoop._handle_events(13, 1)>)\n    120             self._callback = None\n    121             self._args = None\n    122 \n    123     def _run(self):\n    124         try:\n--> 125             self._callback(*self._args)\n        self._callback = <bound method BaseAsyncIOLoop._handle_events of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n        self._args = (13, 1)\n    126         except Exception as exc:\n    127             cb = _format_callback_source(self._callback, self._args)\n    128             msg = 'Exception in callback {}'.format(cb)\n    129             context = {\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py in _handle_events(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, fd=13, events=1)\n     97             self.writers.remove(fd)\n     98         del self.handlers[fd]\n     99 \n    100     def _handle_events(self, fd, events):\n    101         fileobj, handler_func = self.handlers[fd]\n--> 102         handler_func(fileobj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fileobj = <zmq.sugar.socket.Socket object>\n        events = 1\n    103 \n    104     def start(self):\n    105         try:\n    106             old_loop = asyncio.get_event_loop()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    445             return\n    446         zmq_events = self.socket.EVENTS\n    447         try:\n    448             # dispatch events:\n    449             if zmq_events & zmq.POLLIN and self.receiving():\n--> 450                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    451                 if not self.socket:\n    452                     return\n    453             if zmq_events & zmq.POLLOUT and self.sending():\n    454                 self._handle_send()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    475             else:\n    476                 raise\n    477         else:\n    478             if self._recv_callback:\n    479                 callback = self._recv_callback\n--> 480                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    481         \n    482 \n    483     def _handle_send(self):\n    484         \"\"\"Handle a send event.\"\"\"\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    427         close our socket.\"\"\"\n    428         try:\n    429             # Use a NullContext to ensure that all StackContexts are run\n    430             # inside our blanket exception handler rather than outside.\n    431             with stack_context.NullContext():\n--> 432                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    433         except:\n    434             gen_log.error(\"Uncaught exception in ZMQStream callback\",\n    435                           exc_info=True)\n    436             # Re-raise the exception so that IOLoop.handle_callback_exception\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'43d8b9a1122d4f57be7c23f396e940a0']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'43d8b9a1122d4f57be7c23f396e940a0'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>], cell_name='<ipython-input-462-88a493e9dcad>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>)\n   2845 \n   2846         try:\n   2847             for i, node in enumerate(to_run_exec):\n   2848                 mod = ast.Module([node])\n   2849                 code = compiler(mod, cell_name, \"exec\")\n-> 2850                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>\n        result = <ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>, result=<ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>\n        self.user_global_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...m sklearn.cross_validation import cross_val_score', 'features = pd.read_csv(\"features0,0.csv\")', 'randomforesttree = RandomForestClassifier(random...=103040, n_jobs=-1)\\nrandomforesttree.get_params()', 'features.columns', 'def setPred(col):\\n    if(col != 0):\\n        return 1\\n    else:\\n        return 0', '\\n\\nfeatures[\"prediction\"] = setPred(features[\"conversion\"])', 'features = pd.read_csv(\"features0,0.csv\")', 'setPred(features[\"conversion\"])', 'setPred(features[\"conversion\"].values)', 'features[\"conversion\"].apply(setPred)', 'features[\"conversion\"].apply(setPred).sum()', 'features[\"conversion\"]', 'features = pd.read_csv(\"features0,0.csv\")', 'features[\"prediction\"] = features[\"conversion\"]', 'features[\"prediction\"].mean()', 'features[\"prediction\"].max()', 'features[\"prediction\"] = features[\"conversion\"].applu(setPred)', 'features[\"prediction\"] = features[\"conversion\"].apply(setPred)', 'features[\"prediction\"].max()', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {3: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, 4: Index(['Unnamed: 0', 'ad campaign hit', 'brand l...   'nombre mas usado dia'],\n      dtype='object'), 10: 0       1\n1       1\n2       1\n3       1\n4       ...   1\nName: conversion, Length: 1712, dtype: int64, 11: 1712, 12: 0       2.0\n1       1.0\n2       1.0\n3       1.0\n....0\nName: conversion, Length: 1712, dtype: float64, 15: 0.1853565453785027, 16: 129.0, 19: 1, 21: 0.11221769134253451, 22: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, ...}\n        self.user_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...m sklearn.cross_validation import cross_val_score', 'features = pd.read_csv(\"features0,0.csv\")', 'randomforesttree = RandomForestClassifier(random...=103040, n_jobs=-1)\\nrandomforesttree.get_params()', 'features.columns', 'def setPred(col):\\n    if(col != 0):\\n        return 1\\n    else:\\n        return 0', '\\n\\nfeatures[\"prediction\"] = setPred(features[\"conversion\"])', 'features = pd.read_csv(\"features0,0.csv\")', 'setPred(features[\"conversion\"])', 'setPred(features[\"conversion\"].values)', 'features[\"conversion\"].apply(setPred)', 'features[\"conversion\"].apply(setPred).sum()', 'features[\"conversion\"]', 'features = pd.read_csv(\"features0,0.csv\")', 'features[\"prediction\"] = features[\"conversion\"]', 'features[\"prediction\"].mean()', 'features[\"prediction\"].max()', 'features[\"prediction\"] = features[\"conversion\"].applu(setPred)', 'features[\"prediction\"] = features[\"conversion\"].apply(setPred)', 'features[\"prediction\"].max()', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {3: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, 4: Index(['Unnamed: 0', 'ad campaign hit', 'brand l...   'nombre mas usado dia'],\n      dtype='object'), 10: 0       1\n1       1\n2       1\n3       1\n4       ...   1\nName: conversion, Length: 1712, dtype: int64, 11: 1712, 12: 0       2.0\n1       1.0\n2       1.0\n3       1.0\n....0\nName: conversion, Length: 1712, dtype: float64, 15: 0.1853565453785027, 16: 129.0, 19: 1, 21: 0.11221769134253451, 22: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\n/home/tito/Desktop/datas/gitNuevo/<ipython-input-462-88a493e9dcad> in <module>()\n      1 \n      2 \n----> 3 \n      4 param_grid= {'max_features': [\"auto\"], 'n_estimators': [100], 'criterion': ['mse',\"mae\"]}\n      5 grid_drop = GridSearchCV(randomforesttree, param_grid, cv=20, scoring='roc_auc',n_jobs = -1,pre_dispatch = 4)\n      6 grid_drop.fit(X_train, y_train)\n      7 print(grid_drop.best_params_, grid_drop.best_score_)\n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=20, error_score='raise',\n       ...atch=4, refit=True, scoring='roc_auc', verbose=0), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64)\n    799         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n    800             Target relative to X for classification or regression;\n    801             None for unsupervised learning.\n    802 \n    803         \"\"\"\n--> 804         return self._fit(X, y, ParameterGrid(self.param_grid))\n        self._fit = <bound method BaseSearchCV._fit of GridSearchCV(...tch=4, refit=True, scoring='roc_auc', verbose=0)>\n        X =        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns]\n        y = 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64\n        self.param_grid = {'criterion': ['mse', 'mae'], 'max_features': ['auto'], 'n_estimators': [100]}\n    805 \n    806 \n    807 class RandomizedSearchCV(BaseSearchCV):\n    808     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=20, error_score='raise',\n       ...atch=4, refit=True, scoring='roc_auc', verbose=0), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\n    548         )(\n    549             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    550                                     train, test, self.verbose, parameters,\n    551                                     self.fit_params, return_parameters=True,\n    552                                     error_score=self.error_score)\n--> 553                 for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterGrid object>\n    554                 for train, test in cv)\n    555 \n    556         # Out is a list of triplet: score, estimator, n_test_samples\n    557         n_fits = len(out)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV._fit.<locals>.<genexpr>>)\n    761             raise ValueError('This Parallel instance is already running')\n    762         # A flag used to abort the dispatching of jobs in case an\n    763         # exception is found\n    764         self._aborting = False\n    765         if not self._managed_pool:\n--> 766             n_jobs = self._initialize_pool()\n        n_jobs = undefined\n        self._initialize_pool = <bound method Parallel._initialize_pool of Parallel(n_jobs=-1)>\n    767         else:\n    768             n_jobs = self._effective_n_jobs()\n    769 \n    770         if self.batch_size == 'auto':\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in _initialize_pool(self=Parallel(n_jobs=-1))\n    532                     verbose=max(0, self.verbose - 50),\n    533                 )\n    534                 if self._mp_context is not None:\n    535                     # Use Python 3.4+ multiprocessing context isolation\n    536                     poolargs['context'] = self._mp_context\n--> 537                 self._pool = MemmapingPool(n_jobs, **poolargs)\n        self._pool = None\n        n_jobs = 4\n        poolargs = {'context': <multiprocessing.context.ForkContext object>, 'max_nbytes': 1048576, 'mmap_mode': 'r', 'temp_folder': None, 'verbose': 0}\n    538 \n    539                 # We are using multiprocessing, we also want to capture\n    540                 # KeyboardInterrupts\n    541                 self.exceptions.extend([KeyboardInterrupt, WorkerInterrupt])\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/pool.py in __init__(self=<joblib.pool.MemmapingPool object>, processes=4, temp_folder='/dev/shm', max_nbytes=1048576, mmap_mode='r', forward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, backward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, verbose=0, context_id=None, prewarm=False, **kwargs={'context': <multiprocessing.context.ForkContext object>})\n    575         poolargs = dict(\n    576             processes=processes,\n    577             forward_reducers=forward_reducers,\n    578             backward_reducers=backward_reducers)\n    579         poolargs.update(kwargs)\n--> 580         super(MemmapingPool, self).__init__(**poolargs)\n        self.__init__ = <bound method MemmapingPool.__init__ of <joblib.pool.MemmapingPool object>>\n        poolargs = {'backward_reducers': {<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, 'context': <multiprocessing.context.ForkContext object>, 'forward_reducers': {<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, 'processes': 4}\n    581 \n    582     def terminate(self):\n    583         super(MemmapingPool, self).terminate()\n    584         delete_folder(self._temp_folder)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/pool.py in __init__(self=<joblib.pool.MemmapingPool object>, processes=4, forward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, backward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, **kwargs={'context': <multiprocessing.context.ForkContext object>})\n    413             backward_reducers = dict()\n    414         self._forward_reducers = forward_reducers\n    415         self._backward_reducers = backward_reducers\n    416         poolargs = dict(processes=processes)\n    417         poolargs.update(kwargs)\n--> 418         super(PicklingPool, self).__init__(**poolargs)\n        self.__init__ = <bound method MemmapingPool.__init__ of <joblib.pool.MemmapingPool object>>\n        poolargs = {'context': <multiprocessing.context.ForkContext object>, 'processes': 4}\n    419 \n    420     def _setup_queues(self):\n    421         context = getattr(self, '_ctx', mp)\n    422         self._inqueue = CustomizablePicklingQueue(context,\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/pool.py in __init__(self=<joblib.pool.MemmapingPool object>, processes=4, initializer=None, initargs=(), maxtasksperchild=None, context=<multiprocessing.context.ForkContext object>)\n    163         if initializer is not None and not callable(initializer):\n    164             raise TypeError('initializer must be a callable')\n    165 \n    166         self._processes = processes\n    167         self._pool = []\n--> 168         self._repopulate_pool()\n        self._repopulate_pool = <bound method Pool._repopulate_pool of <joblib.pool.MemmapingPool object>>\n    169 \n    170         self._worker_handler = threading.Thread(\n    171             target=Pool._handle_workers,\n    172             args=(self, )\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/pool.py in _repopulate_pool(self=<joblib.pool.MemmapingPool object>)\n    228                                    self._wrap_exception)\n    229                             )\n    230             self._pool.append(w)\n    231             w.name = w.name.replace('Process', 'PoolWorker')\n    232             w.daemon = True\n--> 233             w.start()\n        w.start = <bound method BaseProcess.start of <ForkProcess(ForkPoolWorker-130, started daemon)>>\n    234             util.debug('added worker')\n    235 \n    236     def _maintain_pool(self):\n    237         \"\"\"Clean up any exited workers and start replacements for them.\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/process.py in start(self=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n    100         assert self._parent_pid == os.getpid(), \\\n    101                'can only start a process object created by current process'\n    102         assert not _current_process._config.get('daemon'), \\\n    103                'daemonic processes are not allowed to have children'\n    104         _cleanup()\n--> 105         self._popen = self._Popen(self)\n        self._popen = None\n        self._Popen = <function ForkProcess._Popen>\n        self = <ForkProcess(ForkPoolWorker-130, started daemon)>\n    106         self._sentinel = self._popen.sentinel\n    107         _children.add(self)\n    108 \n    109     def terminate(self):\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/context.py in _Popen(process_obj=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n    262     class ForkProcess(process.BaseProcess):\n    263         _start_method = 'fork'\n    264         @staticmethod\n    265         def _Popen(process_obj):\n    266             from .popen_fork import Popen\n--> 267             return Popen(process_obj)\n        Popen = <class 'multiprocessing.popen_fork.Popen'>\n        process_obj = <ForkProcess(ForkPoolWorker-130, started daemon)>\n    268 \n    269     class SpawnProcess(process.BaseProcess):\n    270         _start_method = 'spawn'\n    271         @staticmethod\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/popen_fork.py in __init__(self=<multiprocessing.popen_fork.Popen object>, process_obj=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n     15 \n     16     def __init__(self, process_obj):\n     17         sys.stdout.flush()\n     18         sys.stderr.flush()\n     19         self.returncode = None\n---> 20         self._launch(process_obj)\n        self._launch = <bound method Popen._launch of <multiprocessing.popen_fork.Popen object>>\n        process_obj = <ForkProcess(ForkPoolWorker-130, started daemon)>\n     21 \n     22     def duplicate_for_child(self, fd):\n     23         return fd\n     24 \n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/popen_fork.py in _launch(self=<multiprocessing.popen_fork.Popen object>, process_obj=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n     69             try:\n     70                 os.close(parent_r)\n     71                 if 'random' in sys.modules:\n     72                     import random\n     73                     random.seed()\n---> 74                 code = process_obj._bootstrap()\n        code = 1\n        process_obj._bootstrap = <bound method BaseProcess._bootstrap of <ForkProcess(ForkPoolWorker-130, started daemon)>>\n     75             finally:\n     76                 os._exit(code)\n     77         else:\n     78             os.close(child_w)\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/process.py in _bootstrap(self=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n    244                 # delay finalization of the old process object until after\n    245                 # _run_after_forkers() is executed\n    246                 del old_process\n    247             util.info('child process calling self.run()')\n    248             try:\n--> 249                 self.run()\n        self.run = <bound method BaseProcess.run of <ForkProcess(ForkPoolWorker-130, started daemon)>>\n    250                 exitcode = 0\n    251             finally:\n    252                 util._exit_function()\n    253         except SystemExit as e:\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/process.py in run(self=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n     88     def run(self):\n     89         '''\n     90         Method to be run in sub-process; can be overridden in sub-class\n     91         '''\n     92         if self._target:\n---> 93             self._target(*self._args, **self._kwargs)\n        self._target = <function worker>\n        self._args = (<joblib.pool.CustomizablePicklingQueue object>, <joblib.pool.CustomizablePicklingQueue object>, None, (), None, True)\n        self._kwargs = {}\n     94 \n     95     def start(self):\n     96         '''\n     97         Start child process\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/pool.py in worker(inqueue=<joblib.pool.CustomizablePicklingQueue object>, outqueue=<joblib.pool.CustomizablePicklingQueue object>, initializer=None, initargs=(), maxtasks=None, wrap_exception=True)\n    114             util.debug('worker got sentinel -- exiting')\n    115             break\n    116 \n    117         job, i, func, args, kwds = task\n    118         try:\n--> 119             result = (True, func(*args, **kwds))\n        result = (True, [[0.7652468076996379, 669, 25.73760676383972, {'criterion': 'mse', 'max_features': 'auto', 'n_estimators': 100}]])\n        func = <joblib.parallel.SafeFunction object>\n        args = ()\n        kwds = {}\n    120         except Exception as e:\n    121             if wrap_exception:\n    122                 e = ExceptionWithTraceback(e, e.__traceback__)\n    123             result = (False, e)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.SafeFunction object>, *args=(), **kwargs={})\n    125     def __init__(self, func):\n    126         self.func = func\n    127 \n    128     def __call__(self, *args, **kwargs):\n    129         try:\n--> 130             return self.func(*args, **kwargs)\n        self.func = <joblib.parallel.BatchedCalls object>\n        args = ()\n        kwargs = {}\n    131         except KeyboardInterrupt:\n    132             # We capture the KeyboardInterrupt and reraise it as\n    133             # something different, as multiprocessing does not\n    134             # interrupt processing for a KeyboardInterrupt\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {}), {'error_score': 'raise', 'return_parameters': True})]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/cross_validation.py in _fit_and_score(estimator=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, scorer=make_scorer(roc_auc_score, needs_threshold=True), train=array([  670,   671,   672, ..., 13386, 13387, 13388]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), verbose=0, parameters={'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1526 \n   1527     try:\n   1528         if y_train is None:\n   1529             estimator.fit(X_train, **fit_params)\n   1530         else:\n-> 1531             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestRegr...te=None,\n           verbose=0, warm_start=False)>\n        X_train =        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[12719 rows x 17 columns]\n        y_train = 6507     0\n12276    0\n4391     0\n18119    0\n9744...024    0\nName: label, Length: 12719, dtype: int64\n        fit_params = {}\n   1532 \n   1533     except Exception as e:\n   1534         if error_score == 'raise':\n   1535             raise\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py in fit(self=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=None)\n    285             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    286                              backend=\"threading\")(\n    287                 delayed(_parallel_build_trees)(\n    288                     t, self, X, y, sample_weight, i, len(trees),\n    289                     verbose=self.verbose, class_weight=self.class_weight)\n--> 290                 for i, t in enumerate(trees))\n        i = 99\n    291 \n    292             # Collect newly grown trees\n    293             self.estimators_.extend(trees)\n    294 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    805             if pre_dispatch == \"all\" or n_jobs == 1:\n    806                 # The iterable was consumed all at once by the above for loop.\n    807                 # No need to wait for async callbacks to trigger to\n    808                 # consumption.\n    809                 self._iterating = False\n--> 810             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    811             # Make sure that we get a last message telling us we are done\n    812             elapsed_time = time.time() - self._start_time\n    813             self._print('Done %3i out of %3i | elapsed: %s finished',\n    814                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nKeyError                                           Fri Nov  9 23:18:19 2018\nPID: 13242                                   Python 3.5.2: /usr/bin/python3\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _parallel_build_trees>, (DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), None, 0, 100), {'class_weight': None, 'verbose': 0})]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _parallel_build_trees>\n        args = (DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), None, 0, 100)\n        kwargs = {'class_weight': None, 'verbose': 0}\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py in _parallel_build_trees(tree=DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), forest=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=None, tree_idx=0, n_trees=100, verbose=0, class_weight=None)\n    111                 warnings.simplefilter('ignore', DeprecationWarning)\n    112                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n    113         elif class_weight == 'balanced_subsample':\n    114             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n    115 \n--> 116         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n        tree.fit = <bound method BaseDecisionTree.fit of DecisionTr...         random_state=19780174, splitter='best')>\n        X = array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32)\n        y = array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]])\n        sample_weight = None\n        curr_sample_weight = array([1., 1., 0., ..., 0., 2., 1.])\n    117     else:\n    118         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    119 \n    120     return tree\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/tree/tree.py in fit(self=DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=array([1., 1., 0., ..., 0., 2., 1.]), check_input=False, X_idx_sorted=None)\n    317         if not isinstance(criterion, Criterion):\n    318             if is_classification:\n    319                 criterion = CRITERIA_CLF[self.criterion](self.n_outputs_,\n    320                                                          self.n_classes_)\n    321             else:\n--> 322                 criterion = CRITERIA_REG[self.criterion](self.n_outputs_)\n        criterion = 'mae'\n        self.criterion = 'mae'\n        self.n_outputs_ = 1\n    323 \n    324         SPLITTERS = SPARSE_SPLITTERS if issparse(X) else DENSE_SPLITTERS\n    325 \n    326         splitter = self.splitter\n\nKeyError: 'mae'\n___________________________________________________________________________\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3/dist-packages/joblib/parallel.py\", line 140, in __call__\n    raise TransportableException(text, e_type)\njoblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nJoblibKeyError                                     Fri Nov  9 23:18:21 2018\nPID: 13242                                   Python 3.5.2: /usr/bin/python3\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {}), {'error_score': 'raise', 'return_parameters': True})]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/cross_validation.py in _fit_and_score(estimator=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, scorer=make_scorer(roc_auc_score, needs_threshold=True), train=array([  670,   671,   672, ..., 13386, 13387, 13388]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), verbose=0, parameters={'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1526 \n   1527     try:\n   1528         if y_train is None:\n   1529             estimator.fit(X_train, **fit_params)\n   1530         else:\n-> 1531             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestRegr...te=None,\n           verbose=0, warm_start=False)>\n        X_train =        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[12719 rows x 17 columns]\n        y_train = 6507     0\n12276    0\n4391     0\n18119    0\n9744...024    0\nName: label, Length: 12719, dtype: int64\n        fit_params = {}\n   1532 \n   1533     except Exception as e:\n   1534         if error_score == 'raise':\n   1535             raise\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py in fit(self=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=None)\n    285             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    286                              backend=\"threading\")(\n    287                 delayed(_parallel_build_trees)(\n    288                     t, self, X, y, sample_weight, i, len(trees),\n    289                     verbose=self.verbose, class_weight=self.class_weight)\n--> 290                 for i, t in enumerate(trees))\n        i = 99\n    291 \n    292             # Collect newly grown trees\n    293             self.estimators_.extend(trees)\n    294 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    805             if pre_dispatch == \"all\" or n_jobs == 1:\n    806                 # The iterable was consumed all at once by the above for loop.\n    807                 # No need to wait for async callbacks to trigger to\n    808                 # consumption.\n    809                 self._iterating = False\n--> 810             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    811             # Make sure that we get a last message telling us we are done\n    812             elapsed_time = time.time() - self._start_time\n    813             self._print('Done %3i out of %3i | elapsed: %s finished',\n    814                         (len(self._output), len(self._output),\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in retrieve(self=Parallel(n_jobs=-1))\n    752                     # In case we had to terminate a managed pool, let\n    753                     # us start a new one to ensure that subsequent calls\n    754                     # to __call__ on the same Parallel instance will get\n    755                     # a working pool as they expect.\n    756                     self._initialize_pool()\n--> 757                 raise exception\n        exception = undefined\n    758 \n    759     def __call__(self, iterable):\n    760         if self._jobs:\n    761             raise ValueError('This Parallel instance is already running')\n\nJoblibKeyError: JoblibKeyError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python3.5/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    179         sys.exit(msg)\n    180     main_globals = sys.modules[\"__main__\"].__dict__\n    181     if alter_argv:\n    182         sys.argv[0] = mod_spec.origin\n    183     return _run_code(code, main_globals, None,\n--> 184                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py')\n    185 \n    186 def run_module(mod_name, init_globals=None,\n    187                run_name=None, alter_sys=False):\n    188     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python3.5/runpy.py in _run_code(code=<code object <module> at 0x7f4e7a34cc90, file \"/...3.5/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/tito/.local/lib/python3.5/site-packages/__pycache__/ipykernel_launcher.cpython-35.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/tito/..../python3.5/site-packages/ipykernel/kernelapp.py'>, 'sys': <module 'sys' (built-in)>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f4e7a34cc90, file \"/...3.5/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/tito/.local/lib/python3.5/site-packages/__pycache__/ipykernel_launcher.cpython-35.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/tito/..../python3.5/site-packages/ipykernel/kernelapp.py'>, 'sys': <module 'sys' (built-in)>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n     17 \n     18 \n     19 \n     20 \n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    481         if self.poller is not None:\n    482             self.poller.start()\n    483         self.kernel.start()\n    484         self.io_loop = ioloop.IOLoop.current()\n    485         try:\n--> 486             self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    487         except KeyboardInterrupt:\n    488             pass\n    489 \n    490 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    107         except RuntimeError:\n    108             old_loop = None\n    109         try:\n    110             self._setup_logging()\n    111             asyncio.set_event_loop(self.asyncio_loop)\n--> 112             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Uni...EventLoop running=True closed=False debug=False>>\n    113         finally:\n    114             asyncio.set_event_loop(old_loop)\n    115 \n    116     def stop(self):\n\n...........................................................................\n/usr/lib/python3.5/asyncio/base_events.py in run_forever(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n    340             raise RuntimeError('Event loop is running.')\n    341         self._set_coroutine_wrapper(self._debug)\n    342         self._thread_id = threading.get_ident()\n    343         try:\n    344             while True:\n--> 345                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_UnixS...EventLoop running=True closed=False debug=False>>\n    346                 if self._stopping:\n    347                     break\n    348         finally:\n    349             self._stopping = False\n\n...........................................................................\n/usr/lib/python3.5/asyncio/base_events.py in _run_once(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n   1307                         logger.warning('Executing %s took %.3f seconds',\n   1308                                        _format_handle(handle), dt)\n   1309                 finally:\n   1310                     self._current_handle = None\n   1311             else:\n-> 1312                 handle._run()\n        handle._run = <bound method Handle._run of <Handle BaseAsyncIOLoop._handle_events(13, 1)>>\n   1313         handle = None  # Needed to break cycles when an exception occurs.\n   1314 \n   1315     def _set_coroutine_wrapper(self, enabled):\n   1316         try:\n\n...........................................................................\n/usr/lib/python3.5/asyncio/events.py in _run(self=<Handle BaseAsyncIOLoop._handle_events(13, 1)>)\n    120             self._callback = None\n    121             self._args = None\n    122 \n    123     def _run(self):\n    124         try:\n--> 125             self._callback(*self._args)\n        self._callback = <bound method BaseAsyncIOLoop._handle_events of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n        self._args = (13, 1)\n    126         except Exception as exc:\n    127             cb = _format_callback_source(self._callback, self._args)\n    128             msg = 'Exception in callback {}'.format(cb)\n    129             context = {\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py in _handle_events(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, fd=13, events=1)\n     97             self.writers.remove(fd)\n     98         del self.handlers[fd]\n     99 \n    100     def _handle_events(self, fd, events):\n    101         fileobj, handler_func = self.handlers[fd]\n--> 102         handler_func(fileobj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fileobj = <zmq.sugar.socket.Socket object>\n        events = 1\n    103 \n    104     def start(self):\n    105         try:\n    106             old_loop = asyncio.get_event_loop()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    445             return\n    446         zmq_events = self.socket.EVENTS\n    447         try:\n    448             # dispatch events:\n    449             if zmq_events & zmq.POLLIN and self.receiving():\n--> 450                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    451                 if not self.socket:\n    452                     return\n    453             if zmq_events & zmq.POLLOUT and self.sending():\n    454                 self._handle_send()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    475             else:\n    476                 raise\n    477         else:\n    478             if self._recv_callback:\n    479                 callback = self._recv_callback\n--> 480                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    481         \n    482 \n    483     def _handle_send(self):\n    484         \"\"\"Handle a send event.\"\"\"\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    427         close our socket.\"\"\"\n    428         try:\n    429             # Use a NullContext to ensure that all StackContexts are run\n    430             # inside our blanket exception handler rather than outside.\n    431             with stack_context.NullContext():\n--> 432                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    433         except:\n    434             gen_log.error(\"Uncaught exception in ZMQStream callback\",\n    435                           exc_info=True)\n    436             # Re-raise the exception so that IOLoop.handle_callback_exception\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'43d8b9a1122d4f57be7c23f396e940a0']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'43d8b9a1122d4f57be7c23f396e940a0'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>], cell_name='<ipython-input-462-88a493e9dcad>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>)\n   2845 \n   2846         try:\n   2847             for i, node in enumerate(to_run_exec):\n   2848                 mod = ast.Module([node])\n   2849                 code = compiler(mod, cell_name, \"exec\")\n-> 2850                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>\n        result = <ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>, result=<ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>\n        self.user_global_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...m sklearn.cross_validation import cross_val_score', 'features = pd.read_csv(\"features0,0.csv\")', 'randomforesttree = RandomForestClassifier(random...=103040, n_jobs=-1)\\nrandomforesttree.get_params()', 'features.columns', 'def setPred(col):\\n    if(col != 0):\\n        return 1\\n    else:\\n        return 0', '\\n\\nfeatures[\"prediction\"] = setPred(features[\"conversion\"])', 'features = pd.read_csv(\"features0,0.csv\")', 'setPred(features[\"conversion\"])', 'setPred(features[\"conversion\"].values)', 'features[\"conversion\"].apply(setPred)', 'features[\"conversion\"].apply(setPred).sum()', 'features[\"conversion\"]', 'features = pd.read_csv(\"features0,0.csv\")', 'features[\"prediction\"] = features[\"conversion\"]', 'features[\"prediction\"].mean()', 'features[\"prediction\"].max()', 'features[\"prediction\"] = features[\"conversion\"].applu(setPred)', 'features[\"prediction\"] = features[\"conversion\"].apply(setPred)', 'features[\"prediction\"].max()', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {3: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, 4: Index(['Unnamed: 0', 'ad campaign hit', 'brand l...   'nombre mas usado dia'],\n      dtype='object'), 10: 0       1\n1       1\n2       1\n3       1\n4       ...   1\nName: conversion, Length: 1712, dtype: int64, 11: 1712, 12: 0       2.0\n1       1.0\n2       1.0\n3       1.0\n....0\nName: conversion, Length: 1712, dtype: float64, 15: 0.1853565453785027, 16: 129.0, 19: 1, 21: 0.11221769134253451, 22: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, ...}\n        self.user_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...m sklearn.cross_validation import cross_val_score', 'features = pd.read_csv(\"features0,0.csv\")', 'randomforesttree = RandomForestClassifier(random...=103040, n_jobs=-1)\\nrandomforesttree.get_params()', 'features.columns', 'def setPred(col):\\n    if(col != 0):\\n        return 1\\n    else:\\n        return 0', '\\n\\nfeatures[\"prediction\"] = setPred(features[\"conversion\"])', 'features = pd.read_csv(\"features0,0.csv\")', 'setPred(features[\"conversion\"])', 'setPred(features[\"conversion\"].values)', 'features[\"conversion\"].apply(setPred)', 'features[\"conversion\"].apply(setPred).sum()', 'features[\"conversion\"]', 'features = pd.read_csv(\"features0,0.csv\")', 'features[\"prediction\"] = features[\"conversion\"]', 'features[\"prediction\"].mean()', 'features[\"prediction\"].max()', 'features[\"prediction\"] = features[\"conversion\"].applu(setPred)', 'features[\"prediction\"] = features[\"conversion\"].apply(setPred)', 'features[\"prediction\"].max()', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {3: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, 4: Index(['Unnamed: 0', 'ad campaign hit', 'brand l...   'nombre mas usado dia'],\n      dtype='object'), 10: 0       1\n1       1\n2       1\n3       1\n4       ...   1\nName: conversion, Length: 1712, dtype: int64, 11: 1712, 12: 0       2.0\n1       1.0\n2       1.0\n3       1.0\n....0\nName: conversion, Length: 1712, dtype: float64, 15: 0.1853565453785027, 16: 129.0, 19: 1, 21: 0.11221769134253451, 22: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\n/home/tito/Desktop/datas/gitNuevo/<ipython-input-462-88a493e9dcad> in <module>()\n      1 \n      2 \n----> 3 \n      4 param_grid= {'max_features': [\"auto\"], 'n_estimators': [100], 'criterion': ['mse',\"mae\"]}\n      5 grid_drop = GridSearchCV(randomforesttree, param_grid, cv=20, scoring='roc_auc',n_jobs = -1,pre_dispatch = 4)\n      6 grid_drop.fit(X_train, y_train)\n      7 print(grid_drop.best_params_, grid_drop.best_score_)\n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=20, error_score='raise',\n       ...atch=4, refit=True, scoring='roc_auc', verbose=0), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64)\n    799         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n    800             Target relative to X for classification or regression;\n    801             None for unsupervised learning.\n    802 \n    803         \"\"\"\n--> 804         return self._fit(X, y, ParameterGrid(self.param_grid))\n        self._fit = <bound method BaseSearchCV._fit of GridSearchCV(...tch=4, refit=True, scoring='roc_auc', verbose=0)>\n        X =        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns]\n        y = 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64\n        self.param_grid = {'criterion': ['mse', 'mae'], 'max_features': ['auto'], 'n_estimators': [100]}\n    805 \n    806 \n    807 class RandomizedSearchCV(BaseSearchCV):\n    808     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=20, error_score='raise',\n       ...atch=4, refit=True, scoring='roc_auc', verbose=0), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\n    548         )(\n    549             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    550                                     train, test, self.verbose, parameters,\n    551                                     self.fit_params, return_parameters=True,\n    552                                     error_score=self.error_score)\n--> 553                 for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterGrid object>\n    554                 for train, test in cv)\n    555 \n    556         # Out is a list of triplet: score, estimator, n_test_samples\n    557         n_fits = len(out)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV._fit.<locals>.<genexpr>>)\n    761             raise ValueError('This Parallel instance is already running')\n    762         # A flag used to abort the dispatching of jobs in case an\n    763         # exception is found\n    764         self._aborting = False\n    765         if not self._managed_pool:\n--> 766             n_jobs = self._initialize_pool()\n        n_jobs = undefined\n        self._initialize_pool = <bound method Parallel._initialize_pool of Parallel(n_jobs=-1)>\n    767         else:\n    768             n_jobs = self._effective_n_jobs()\n    769 \n    770         if self.batch_size == 'auto':\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in _initialize_pool(self=Parallel(n_jobs=-1))\n    532                     verbose=max(0, self.verbose - 50),\n    533                 )\n    534                 if self._mp_context is not None:\n    535                     # Use Python 3.4+ multiprocessing context isolation\n    536                     poolargs['context'] = self._mp_context\n--> 537                 self._pool = MemmapingPool(n_jobs, **poolargs)\n        self._pool = None\n        n_jobs = 4\n        poolargs = {'context': <multiprocessing.context.ForkContext object>, 'max_nbytes': 1048576, 'mmap_mode': 'r', 'temp_folder': None, 'verbose': 0}\n    538 \n    539                 # We are using multiprocessing, we also want to capture\n    540                 # KeyboardInterrupts\n    541                 self.exceptions.extend([KeyboardInterrupt, WorkerInterrupt])\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/pool.py in __init__(self=<joblib.pool.MemmapingPool object>, processes=4, temp_folder='/dev/shm', max_nbytes=1048576, mmap_mode='r', forward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, backward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, verbose=0, context_id=None, prewarm=False, **kwargs={'context': <multiprocessing.context.ForkContext object>})\n    575         poolargs = dict(\n    576             processes=processes,\n    577             forward_reducers=forward_reducers,\n    578             backward_reducers=backward_reducers)\n    579         poolargs.update(kwargs)\n--> 580         super(MemmapingPool, self).__init__(**poolargs)\n        self.__init__ = <bound method MemmapingPool.__init__ of <joblib.pool.MemmapingPool object>>\n        poolargs = {'backward_reducers': {<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, 'context': <multiprocessing.context.ForkContext object>, 'forward_reducers': {<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, 'processes': 4}\n    581 \n    582     def terminate(self):\n    583         super(MemmapingPool, self).terminate()\n    584         delete_folder(self._temp_folder)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/pool.py in __init__(self=<joblib.pool.MemmapingPool object>, processes=4, forward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, backward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, **kwargs={'context': <multiprocessing.context.ForkContext object>})\n    413             backward_reducers = dict()\n    414         self._forward_reducers = forward_reducers\n    415         self._backward_reducers = backward_reducers\n    416         poolargs = dict(processes=processes)\n    417         poolargs.update(kwargs)\n--> 418         super(PicklingPool, self).__init__(**poolargs)\n        self.__init__ = <bound method MemmapingPool.__init__ of <joblib.pool.MemmapingPool object>>\n        poolargs = {'context': <multiprocessing.context.ForkContext object>, 'processes': 4}\n    419 \n    420     def _setup_queues(self):\n    421         context = getattr(self, '_ctx', mp)\n    422         self._inqueue = CustomizablePicklingQueue(context,\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/pool.py in __init__(self=<joblib.pool.MemmapingPool object>, processes=4, initializer=None, initargs=(), maxtasksperchild=None, context=<multiprocessing.context.ForkContext object>)\n    163         if initializer is not None and not callable(initializer):\n    164             raise TypeError('initializer must be a callable')\n    165 \n    166         self._processes = processes\n    167         self._pool = []\n--> 168         self._repopulate_pool()\n        self._repopulate_pool = <bound method Pool._repopulate_pool of <joblib.pool.MemmapingPool object>>\n    169 \n    170         self._worker_handler = threading.Thread(\n    171             target=Pool._handle_workers,\n    172             args=(self, )\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/pool.py in _repopulate_pool(self=<joblib.pool.MemmapingPool object>)\n    228                                    self._wrap_exception)\n    229                             )\n    230             self._pool.append(w)\n    231             w.name = w.name.replace('Process', 'PoolWorker')\n    232             w.daemon = True\n--> 233             w.start()\n        w.start = <bound method BaseProcess.start of <ForkProcess(ForkPoolWorker-130, started daemon)>>\n    234             util.debug('added worker')\n    235 \n    236     def _maintain_pool(self):\n    237         \"\"\"Clean up any exited workers and start replacements for them.\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/process.py in start(self=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n    100         assert self._parent_pid == os.getpid(), \\\n    101                'can only start a process object created by current process'\n    102         assert not _current_process._config.get('daemon'), \\\n    103                'daemonic processes are not allowed to have children'\n    104         _cleanup()\n--> 105         self._popen = self._Popen(self)\n        self._popen = None\n        self._Popen = <function ForkProcess._Popen>\n        self = <ForkProcess(ForkPoolWorker-130, started daemon)>\n    106         self._sentinel = self._popen.sentinel\n    107         _children.add(self)\n    108 \n    109     def terminate(self):\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/context.py in _Popen(process_obj=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n    262     class ForkProcess(process.BaseProcess):\n    263         _start_method = 'fork'\n    264         @staticmethod\n    265         def _Popen(process_obj):\n    266             from .popen_fork import Popen\n--> 267             return Popen(process_obj)\n        Popen = <class 'multiprocessing.popen_fork.Popen'>\n        process_obj = <ForkProcess(ForkPoolWorker-130, started daemon)>\n    268 \n    269     class SpawnProcess(process.BaseProcess):\n    270         _start_method = 'spawn'\n    271         @staticmethod\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/popen_fork.py in __init__(self=<multiprocessing.popen_fork.Popen object>, process_obj=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n     15 \n     16     def __init__(self, process_obj):\n     17         sys.stdout.flush()\n     18         sys.stderr.flush()\n     19         self.returncode = None\n---> 20         self._launch(process_obj)\n        self._launch = <bound method Popen._launch of <multiprocessing.popen_fork.Popen object>>\n        process_obj = <ForkProcess(ForkPoolWorker-130, started daemon)>\n     21 \n     22     def duplicate_for_child(self, fd):\n     23         return fd\n     24 \n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/popen_fork.py in _launch(self=<multiprocessing.popen_fork.Popen object>, process_obj=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n     69             try:\n     70                 os.close(parent_r)\n     71                 if 'random' in sys.modules:\n     72                     import random\n     73                     random.seed()\n---> 74                 code = process_obj._bootstrap()\n        code = 1\n        process_obj._bootstrap = <bound method BaseProcess._bootstrap of <ForkProcess(ForkPoolWorker-130, started daemon)>>\n     75             finally:\n     76                 os._exit(code)\n     77         else:\n     78             os.close(child_w)\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/process.py in _bootstrap(self=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n    244                 # delay finalization of the old process object until after\n    245                 # _run_after_forkers() is executed\n    246                 del old_process\n    247             util.info('child process calling self.run()')\n    248             try:\n--> 249                 self.run()\n        self.run = <bound method BaseProcess.run of <ForkProcess(ForkPoolWorker-130, started daemon)>>\n    250                 exitcode = 0\n    251             finally:\n    252                 util._exit_function()\n    253         except SystemExit as e:\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/process.py in run(self=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n     88     def run(self):\n     89         '''\n     90         Method to be run in sub-process; can be overridden in sub-class\n     91         '''\n     92         if self._target:\n---> 93             self._target(*self._args, **self._kwargs)\n        self._target = <function worker>\n        self._args = (<joblib.pool.CustomizablePicklingQueue object>, <joblib.pool.CustomizablePicklingQueue object>, None, (), None, True)\n        self._kwargs = {}\n     94 \n     95     def start(self):\n     96         '''\n     97         Start child process\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/pool.py in worker(inqueue=<joblib.pool.CustomizablePicklingQueue object>, outqueue=<joblib.pool.CustomizablePicklingQueue object>, initializer=None, initargs=(), maxtasks=None, wrap_exception=True)\n    114             util.debug('worker got sentinel -- exiting')\n    115             break\n    116 \n    117         job, i, func, args, kwds = task\n    118         try:\n--> 119             result = (True, func(*args, **kwds))\n        result = (True, [[0.7652468076996379, 669, 25.73760676383972, {'criterion': 'mse', 'max_features': 'auto', 'n_estimators': 100}]])\n        func = <joblib.parallel.SafeFunction object>\n        args = ()\n        kwds = {}\n    120         except Exception as e:\n    121             if wrap_exception:\n    122                 e = ExceptionWithTraceback(e, e.__traceback__)\n    123             result = (False, e)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.SafeFunction object>, *args=(), **kwargs={})\n    125     def __init__(self, func):\n    126         self.func = func\n    127 \n    128     def __call__(self, *args, **kwargs):\n    129         try:\n--> 130             return self.func(*args, **kwargs)\n        self.func = <joblib.parallel.BatchedCalls object>\n        args = ()\n        kwargs = {}\n    131         except KeyboardInterrupt:\n    132             # We capture the KeyboardInterrupt and reraise it as\n    133             # something different, as multiprocessing does not\n    134             # interrupt processing for a KeyboardInterrupt\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {}), {'error_score': 'raise', 'return_parameters': True})]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/cross_validation.py in _fit_and_score(estimator=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, scorer=make_scorer(roc_auc_score, needs_threshold=True), train=array([  670,   671,   672, ..., 13386, 13387, 13388]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), verbose=0, parameters={'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1526 \n   1527     try:\n   1528         if y_train is None:\n   1529             estimator.fit(X_train, **fit_params)\n   1530         else:\n-> 1531             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestRegr...te=None,\n           verbose=0, warm_start=False)>\n        X_train =        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[12719 rows x 17 columns]\n        y_train = 6507     0\n12276    0\n4391     0\n18119    0\n9744...024    0\nName: label, Length: 12719, dtype: int64\n        fit_params = {}\n   1532 \n   1533     except Exception as e:\n   1534         if error_score == 'raise':\n   1535             raise\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py in fit(self=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=None)\n    285             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    286                              backend=\"threading\")(\n    287                 delayed(_parallel_build_trees)(\n    288                     t, self, X, y, sample_weight, i, len(trees),\n    289                     verbose=self.verbose, class_weight=self.class_weight)\n--> 290                 for i, t in enumerate(trees))\n        i = 99\n    291 \n    292             # Collect newly grown trees\n    293             self.estimators_.extend(trees)\n    294 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    805             if pre_dispatch == \"all\" or n_jobs == 1:\n    806                 # The iterable was consumed all at once by the above for loop.\n    807                 # No need to wait for async callbacks to trigger to\n    808                 # consumption.\n    809                 self._iterating = False\n--> 810             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    811             # Make sure that we get a last message telling us we are done\n    812             elapsed_time = time.time() - self._start_time\n    813             self._print('Done %3i out of %3i | elapsed: %s finished',\n    814                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nKeyError                                           Fri Nov  9 23:18:19 2018\nPID: 13242                                   Python 3.5.2: /usr/bin/python3\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _parallel_build_trees>, (DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), None, 0, 100), {'class_weight': None, 'verbose': 0})]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _parallel_build_trees>\n        args = (DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), None, 0, 100)\n        kwargs = {'class_weight': None, 'verbose': 0}\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py in _parallel_build_trees(tree=DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), forest=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=None, tree_idx=0, n_trees=100, verbose=0, class_weight=None)\n    111                 warnings.simplefilter('ignore', DeprecationWarning)\n    112                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n    113         elif class_weight == 'balanced_subsample':\n    114             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n    115 \n--> 116         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n        tree.fit = <bound method BaseDecisionTree.fit of DecisionTr...         random_state=19780174, splitter='best')>\n        X = array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32)\n        y = array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]])\n        sample_weight = None\n        curr_sample_weight = array([1., 1., 0., ..., 0., 2., 1.])\n    117     else:\n    118         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    119 \n    120     return tree\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/tree/tree.py in fit(self=DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=array([1., 1., 0., ..., 0., 2., 1.]), check_input=False, X_idx_sorted=None)\n    317         if not isinstance(criterion, Criterion):\n    318             if is_classification:\n    319                 criterion = CRITERIA_CLF[self.criterion](self.n_outputs_,\n    320                                                          self.n_classes_)\n    321             else:\n--> 322                 criterion = CRITERIA_REG[self.criterion](self.n_outputs_)\n        criterion = 'mae'\n        self.criterion = 'mae'\n        self.n_outputs_ = 1\n    323 \n    324         SPLITTERS = SPARSE_SPLITTERS if issparse(X) else DENSE_SPLITTERS\n    325 \n    326         splitter = self.splitter\n\nKeyError: 'mae'\n___________________________________________________________________________\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nJoblibKeyError                                     Fri Nov  9 23:18:21 2018\nPID: 13242                                   Python 3.5.2: /usr/bin/python3\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {}), {'error_score': 'raise', 'return_parameters': True})]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/cross_validation.py in _fit_and_score(estimator=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, scorer=make_scorer(roc_auc_score, needs_threshold=True), train=array([  670,   671,   672, ..., 13386, 13387, 13388]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), verbose=0, parameters={'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1526 \n   1527     try:\n   1528         if y_train is None:\n   1529             estimator.fit(X_train, **fit_params)\n   1530         else:\n-> 1531             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestRegr...te=None,\n           verbose=0, warm_start=False)>\n        X_train =        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[12719 rows x 17 columns]\n        y_train = 6507     0\n12276    0\n4391     0\n18119    0\n9744...024    0\nName: label, Length: 12719, dtype: int64\n        fit_params = {}\n   1532 \n   1533     except Exception as e:\n   1534         if error_score == 'raise':\n   1535             raise\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py in fit(self=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=None)\n    285             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    286                              backend=\"threading\")(\n    287                 delayed(_parallel_build_trees)(\n    288                     t, self, X, y, sample_weight, i, len(trees),\n    289                     verbose=self.verbose, class_weight=self.class_weight)\n--> 290                 for i, t in enumerate(trees))\n        i = 99\n    291 \n    292             # Collect newly grown trees\n    293             self.estimators_.extend(trees)\n    294 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    805             if pre_dispatch == \"all\" or n_jobs == 1:\n    806                 # The iterable was consumed all at once by the above for loop.\n    807                 # No need to wait for async callbacks to trigger to\n    808                 # consumption.\n    809                 self._iterating = False\n--> 810             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    811             # Make sure that we get a last message telling us we are done\n    812             elapsed_time = time.time() - self._start_time\n    813             self._print('Done %3i out of %3i | elapsed: %s finished',\n    814                         (len(self._output), len(self._output),\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in retrieve(self=Parallel(n_jobs=-1))\n    752                     # In case we had to terminate a managed pool, let\n    753                     # us start a new one to ensure that subsequent calls\n    754                     # to __call__ on the same Parallel instance will get\n    755                     # a working pool as they expect.\n    756                     self._initialize_pool()\n--> 757                 raise exception\n        exception = undefined\n    758 \n    759     def __call__(self, iterable):\n    760         if self._jobs:\n    761             raise ValueError('This Parallel instance is already running')\n\nJoblibKeyError: JoblibKeyError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python3.5/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    179         sys.exit(msg)\n    180     main_globals = sys.modules[\"__main__\"].__dict__\n    181     if alter_argv:\n    182         sys.argv[0] = mod_spec.origin\n    183     return _run_code(code, main_globals, None,\n--> 184                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py')\n    185 \n    186 def run_module(mod_name, init_globals=None,\n    187                run_name=None, alter_sys=False):\n    188     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python3.5/runpy.py in _run_code(code=<code object <module> at 0x7f4e7a34cc90, file \"/...3.5/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/tito/.local/lib/python3.5/site-packages/__pycache__/ipykernel_launcher.cpython-35.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/tito/..../python3.5/site-packages/ipykernel/kernelapp.py'>, 'sys': <module 'sys' (built-in)>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f4e7a34cc90, file \"/...3.5/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/tito/.local/lib/python3.5/site-packages/__pycache__/ipykernel_launcher.cpython-35.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/tito/..../python3.5/site-packages/ipykernel/kernelapp.py'>, 'sys': <module 'sys' (built-in)>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n     17 \n     18 \n     19 \n     20 \n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    481         if self.poller is not None:\n    482             self.poller.start()\n    483         self.kernel.start()\n    484         self.io_loop = ioloop.IOLoop.current()\n    485         try:\n--> 486             self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    487         except KeyboardInterrupt:\n    488             pass\n    489 \n    490 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    107         except RuntimeError:\n    108             old_loop = None\n    109         try:\n    110             self._setup_logging()\n    111             asyncio.set_event_loop(self.asyncio_loop)\n--> 112             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Uni...EventLoop running=True closed=False debug=False>>\n    113         finally:\n    114             asyncio.set_event_loop(old_loop)\n    115 \n    116     def stop(self):\n\n...........................................................................\n/usr/lib/python3.5/asyncio/base_events.py in run_forever(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n    340             raise RuntimeError('Event loop is running.')\n    341         self._set_coroutine_wrapper(self._debug)\n    342         self._thread_id = threading.get_ident()\n    343         try:\n    344             while True:\n--> 345                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_UnixS...EventLoop running=True closed=False debug=False>>\n    346                 if self._stopping:\n    347                     break\n    348         finally:\n    349             self._stopping = False\n\n...........................................................................\n/usr/lib/python3.5/asyncio/base_events.py in _run_once(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n   1307                         logger.warning('Executing %s took %.3f seconds',\n   1308                                        _format_handle(handle), dt)\n   1309                 finally:\n   1310                     self._current_handle = None\n   1311             else:\n-> 1312                 handle._run()\n        handle._run = <bound method Handle._run of <Handle BaseAsyncIOLoop._handle_events(13, 1)>>\n   1313         handle = None  # Needed to break cycles when an exception occurs.\n   1314 \n   1315     def _set_coroutine_wrapper(self, enabled):\n   1316         try:\n\n...........................................................................\n/usr/lib/python3.5/asyncio/events.py in _run(self=<Handle BaseAsyncIOLoop._handle_events(13, 1)>)\n    120             self._callback = None\n    121             self._args = None\n    122 \n    123     def _run(self):\n    124         try:\n--> 125             self._callback(*self._args)\n        self._callback = <bound method BaseAsyncIOLoop._handle_events of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n        self._args = (13, 1)\n    126         except Exception as exc:\n    127             cb = _format_callback_source(self._callback, self._args)\n    128             msg = 'Exception in callback {}'.format(cb)\n    129             context = {\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py in _handle_events(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, fd=13, events=1)\n     97             self.writers.remove(fd)\n     98         del self.handlers[fd]\n     99 \n    100     def _handle_events(self, fd, events):\n    101         fileobj, handler_func = self.handlers[fd]\n--> 102         handler_func(fileobj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fileobj = <zmq.sugar.socket.Socket object>\n        events = 1\n    103 \n    104     def start(self):\n    105         try:\n    106             old_loop = asyncio.get_event_loop()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    445             return\n    446         zmq_events = self.socket.EVENTS\n    447         try:\n    448             # dispatch events:\n    449             if zmq_events & zmq.POLLIN and self.receiving():\n--> 450                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    451                 if not self.socket:\n    452                     return\n    453             if zmq_events & zmq.POLLOUT and self.sending():\n    454                 self._handle_send()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    475             else:\n    476                 raise\n    477         else:\n    478             if self._recv_callback:\n    479                 callback = self._recv_callback\n--> 480                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    481         \n    482 \n    483     def _handle_send(self):\n    484         \"\"\"Handle a send event.\"\"\"\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    427         close our socket.\"\"\"\n    428         try:\n    429             # Use a NullContext to ensure that all StackContexts are run\n    430             # inside our blanket exception handler rather than outside.\n    431             with stack_context.NullContext():\n--> 432                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    433         except:\n    434             gen_log.error(\"Uncaught exception in ZMQStream callback\",\n    435                           exc_info=True)\n    436             # Re-raise the exception so that IOLoop.handle_callback_exception\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'43d8b9a1122d4f57be7c23f396e940a0']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'43d8b9a1122d4f57be7c23f396e940a0'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>], cell_name='<ipython-input-462-88a493e9dcad>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>)\n   2845 \n   2846         try:\n   2847             for i, node in enumerate(to_run_exec):\n   2848                 mod = ast.Module([node])\n   2849                 code = compiler(mod, cell_name, \"exec\")\n-> 2850                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>\n        result = <ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>, result=<ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>\n        self.user_global_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...m sklearn.cross_validation import cross_val_score', 'features = pd.read_csv(\"features0,0.csv\")', 'randomforesttree = RandomForestClassifier(random...=103040, n_jobs=-1)\\nrandomforesttree.get_params()', 'features.columns', 'def setPred(col):\\n    if(col != 0):\\n        return 1\\n    else:\\n        return 0', '\\n\\nfeatures[\"prediction\"] = setPred(features[\"conversion\"])', 'features = pd.read_csv(\"features0,0.csv\")', 'setPred(features[\"conversion\"])', 'setPred(features[\"conversion\"].values)', 'features[\"conversion\"].apply(setPred)', 'features[\"conversion\"].apply(setPred).sum()', 'features[\"conversion\"]', 'features = pd.read_csv(\"features0,0.csv\")', 'features[\"prediction\"] = features[\"conversion\"]', 'features[\"prediction\"].mean()', 'features[\"prediction\"].max()', 'features[\"prediction\"] = features[\"conversion\"].applu(setPred)', 'features[\"prediction\"] = features[\"conversion\"].apply(setPred)', 'features[\"prediction\"].max()', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {3: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, 4: Index(['Unnamed: 0', 'ad campaign hit', 'brand l...   'nombre mas usado dia'],\n      dtype='object'), 10: 0       1\n1       1\n2       1\n3       1\n4       ...   1\nName: conversion, Length: 1712, dtype: int64, 11: 1712, 12: 0       2.0\n1       1.0\n2       1.0\n3       1.0\n....0\nName: conversion, Length: 1712, dtype: float64, 15: 0.1853565453785027, 16: 129.0, 19: 1, 21: 0.11221769134253451, 22: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, ...}\n        self.user_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...m sklearn.cross_validation import cross_val_score', 'features = pd.read_csv(\"features0,0.csv\")', 'randomforesttree = RandomForestClassifier(random...=103040, n_jobs=-1)\\nrandomforesttree.get_params()', 'features.columns', 'def setPred(col):\\n    if(col != 0):\\n        return 1\\n    else:\\n        return 0', '\\n\\nfeatures[\"prediction\"] = setPred(features[\"conversion\"])', 'features = pd.read_csv(\"features0,0.csv\")', 'setPred(features[\"conversion\"])', 'setPred(features[\"conversion\"].values)', 'features[\"conversion\"].apply(setPred)', 'features[\"conversion\"].apply(setPred).sum()', 'features[\"conversion\"]', 'features = pd.read_csv(\"features0,0.csv\")', 'features[\"prediction\"] = features[\"conversion\"]', 'features[\"prediction\"].mean()', 'features[\"prediction\"].max()', 'features[\"prediction\"] = features[\"conversion\"].applu(setPred)', 'features[\"prediction\"] = features[\"conversion\"].apply(setPred)', 'features[\"prediction\"].max()', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {3: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, 4: Index(['Unnamed: 0', 'ad campaign hit', 'brand l...   'nombre mas usado dia'],\n      dtype='object'), 10: 0       1\n1       1\n2       1\n3       1\n4       ...   1\nName: conversion, Length: 1712, dtype: int64, 11: 1712, 12: 0       2.0\n1       1.0\n2       1.0\n3       1.0\n....0\nName: conversion, Length: 1712, dtype: float64, 15: 0.1853565453785027, 16: 129.0, 19: 1, 21: 0.11221769134253451, 22: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\n/home/tito/Desktop/datas/gitNuevo/<ipython-input-462-88a493e9dcad> in <module>()\n      1 \n      2 \n----> 3 \n      4 param_grid= {'max_features': [\"auto\"], 'n_estimators': [100], 'criterion': ['mse',\"mae\"]}\n      5 grid_drop = GridSearchCV(randomforesttree, param_grid, cv=20, scoring='roc_auc',n_jobs = -1,pre_dispatch = 4)\n      6 grid_drop.fit(X_train, y_train)\n      7 print(grid_drop.best_params_, grid_drop.best_score_)\n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=20, error_score='raise',\n       ...atch=4, refit=True, scoring='roc_auc', verbose=0), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64)\n    799         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n    800             Target relative to X for classification or regression;\n    801             None for unsupervised learning.\n    802 \n    803         \"\"\"\n--> 804         return self._fit(X, y, ParameterGrid(self.param_grid))\n        self._fit = <bound method BaseSearchCV._fit of GridSearchCV(...tch=4, refit=True, scoring='roc_auc', verbose=0)>\n        X =        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns]\n        y = 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64\n        self.param_grid = {'criterion': ['mse', 'mae'], 'max_features': ['auto'], 'n_estimators': [100]}\n    805 \n    806 \n    807 class RandomizedSearchCV(BaseSearchCV):\n    808     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=20, error_score='raise',\n       ...atch=4, refit=True, scoring='roc_auc', verbose=0), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\n    548         )(\n    549             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    550                                     train, test, self.verbose, parameters,\n    551                                     self.fit_params, return_parameters=True,\n    552                                     error_score=self.error_score)\n--> 553                 for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterGrid object>\n    554                 for train, test in cv)\n    555 \n    556         # Out is a list of triplet: score, estimator, n_test_samples\n    557         n_fits = len(out)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV._fit.<locals>.<genexpr>>)\n    761             raise ValueError('This Parallel instance is already running')\n    762         # A flag used to abort the dispatching of jobs in case an\n    763         # exception is found\n    764         self._aborting = False\n    765         if not self._managed_pool:\n--> 766             n_jobs = self._initialize_pool()\n        n_jobs = undefined\n        self._initialize_pool = <bound method Parallel._initialize_pool of Parallel(n_jobs=-1)>\n    767         else:\n    768             n_jobs = self._effective_n_jobs()\n    769 \n    770         if self.batch_size == 'auto':\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in _initialize_pool(self=Parallel(n_jobs=-1))\n    532                     verbose=max(0, self.verbose - 50),\n    533                 )\n    534                 if self._mp_context is not None:\n    535                     # Use Python 3.4+ multiprocessing context isolation\n    536                     poolargs['context'] = self._mp_context\n--> 537                 self._pool = MemmapingPool(n_jobs, **poolargs)\n        self._pool = None\n        n_jobs = 4\n        poolargs = {'context': <multiprocessing.context.ForkContext object>, 'max_nbytes': 1048576, 'mmap_mode': 'r', 'temp_folder': None, 'verbose': 0}\n    538 \n    539                 # We are using multiprocessing, we also want to capture\n    540                 # KeyboardInterrupts\n    541                 self.exceptions.extend([KeyboardInterrupt, WorkerInterrupt])\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/pool.py in __init__(self=<joblib.pool.MemmapingPool object>, processes=4, temp_folder='/dev/shm', max_nbytes=1048576, mmap_mode='r', forward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, backward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, verbose=0, context_id=None, prewarm=False, **kwargs={'context': <multiprocessing.context.ForkContext object>})\n    575         poolargs = dict(\n    576             processes=processes,\n    577             forward_reducers=forward_reducers,\n    578             backward_reducers=backward_reducers)\n    579         poolargs.update(kwargs)\n--> 580         super(MemmapingPool, self).__init__(**poolargs)\n        self.__init__ = <bound method MemmapingPool.__init__ of <joblib.pool.MemmapingPool object>>\n        poolargs = {'backward_reducers': {<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, 'context': <multiprocessing.context.ForkContext object>, 'forward_reducers': {<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, 'processes': 4}\n    581 \n    582     def terminate(self):\n    583         super(MemmapingPool, self).terminate()\n    584         delete_folder(self._temp_folder)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/pool.py in __init__(self=<joblib.pool.MemmapingPool object>, processes=4, forward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, backward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, **kwargs={'context': <multiprocessing.context.ForkContext object>})\n    413             backward_reducers = dict()\n    414         self._forward_reducers = forward_reducers\n    415         self._backward_reducers = backward_reducers\n    416         poolargs = dict(processes=processes)\n    417         poolargs.update(kwargs)\n--> 418         super(PicklingPool, self).__init__(**poolargs)\n        self.__init__ = <bound method MemmapingPool.__init__ of <joblib.pool.MemmapingPool object>>\n        poolargs = {'context': <multiprocessing.context.ForkContext object>, 'processes': 4}\n    419 \n    420     def _setup_queues(self):\n    421         context = getattr(self, '_ctx', mp)\n    422         self._inqueue = CustomizablePicklingQueue(context,\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/pool.py in __init__(self=<joblib.pool.MemmapingPool object>, processes=4, initializer=None, initargs=(), maxtasksperchild=None, context=<multiprocessing.context.ForkContext object>)\n    163         if initializer is not None and not callable(initializer):\n    164             raise TypeError('initializer must be a callable')\n    165 \n    166         self._processes = processes\n    167         self._pool = []\n--> 168         self._repopulate_pool()\n        self._repopulate_pool = <bound method Pool._repopulate_pool of <joblib.pool.MemmapingPool object>>\n    169 \n    170         self._worker_handler = threading.Thread(\n    171             target=Pool._handle_workers,\n    172             args=(self, )\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/pool.py in _repopulate_pool(self=<joblib.pool.MemmapingPool object>)\n    228                                    self._wrap_exception)\n    229                             )\n    230             self._pool.append(w)\n    231             w.name = w.name.replace('Process', 'PoolWorker')\n    232             w.daemon = True\n--> 233             w.start()\n        w.start = <bound method BaseProcess.start of <ForkProcess(ForkPoolWorker-130, started daemon)>>\n    234             util.debug('added worker')\n    235 \n    236     def _maintain_pool(self):\n    237         \"\"\"Clean up any exited workers and start replacements for them.\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/process.py in start(self=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n    100         assert self._parent_pid == os.getpid(), \\\n    101                'can only start a process object created by current process'\n    102         assert not _current_process._config.get('daemon'), \\\n    103                'daemonic processes are not allowed to have children'\n    104         _cleanup()\n--> 105         self._popen = self._Popen(self)\n        self._popen = None\n        self._Popen = <function ForkProcess._Popen>\n        self = <ForkProcess(ForkPoolWorker-130, started daemon)>\n    106         self._sentinel = self._popen.sentinel\n    107         _children.add(self)\n    108 \n    109     def terminate(self):\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/context.py in _Popen(process_obj=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n    262     class ForkProcess(process.BaseProcess):\n    263         _start_method = 'fork'\n    264         @staticmethod\n    265         def _Popen(process_obj):\n    266             from .popen_fork import Popen\n--> 267             return Popen(process_obj)\n        Popen = <class 'multiprocessing.popen_fork.Popen'>\n        process_obj = <ForkProcess(ForkPoolWorker-130, started daemon)>\n    268 \n    269     class SpawnProcess(process.BaseProcess):\n    270         _start_method = 'spawn'\n    271         @staticmethod\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/popen_fork.py in __init__(self=<multiprocessing.popen_fork.Popen object>, process_obj=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n     15 \n     16     def __init__(self, process_obj):\n     17         sys.stdout.flush()\n     18         sys.stderr.flush()\n     19         self.returncode = None\n---> 20         self._launch(process_obj)\n        self._launch = <bound method Popen._launch of <multiprocessing.popen_fork.Popen object>>\n        process_obj = <ForkProcess(ForkPoolWorker-130, started daemon)>\n     21 \n     22     def duplicate_for_child(self, fd):\n     23         return fd\n     24 \n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/popen_fork.py in _launch(self=<multiprocessing.popen_fork.Popen object>, process_obj=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n     69             try:\n     70                 os.close(parent_r)\n     71                 if 'random' in sys.modules:\n     72                     import random\n     73                     random.seed()\n---> 74                 code = process_obj._bootstrap()\n        code = 1\n        process_obj._bootstrap = <bound method BaseProcess._bootstrap of <ForkProcess(ForkPoolWorker-130, started daemon)>>\n     75             finally:\n     76                 os._exit(code)\n     77         else:\n     78             os.close(child_w)\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/process.py in _bootstrap(self=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n    244                 # delay finalization of the old process object until after\n    245                 # _run_after_forkers() is executed\n    246                 del old_process\n    247             util.info('child process calling self.run()')\n    248             try:\n--> 249                 self.run()\n        self.run = <bound method BaseProcess.run of <ForkProcess(ForkPoolWorker-130, started daemon)>>\n    250                 exitcode = 0\n    251             finally:\n    252                 util._exit_function()\n    253         except SystemExit as e:\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/process.py in run(self=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n     88     def run(self):\n     89         '''\n     90         Method to be run in sub-process; can be overridden in sub-class\n     91         '''\n     92         if self._target:\n---> 93             self._target(*self._args, **self._kwargs)\n        self._target = <function worker>\n        self._args = (<joblib.pool.CustomizablePicklingQueue object>, <joblib.pool.CustomizablePicklingQueue object>, None, (), None, True)\n        self._kwargs = {}\n     94 \n     95     def start(self):\n     96         '''\n     97         Start child process\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/pool.py in worker(inqueue=<joblib.pool.CustomizablePicklingQueue object>, outqueue=<joblib.pool.CustomizablePicklingQueue object>, initializer=None, initargs=(), maxtasks=None, wrap_exception=True)\n    114             util.debug('worker got sentinel -- exiting')\n    115             break\n    116 \n    117         job, i, func, args, kwds = task\n    118         try:\n--> 119             result = (True, func(*args, **kwds))\n        result = (True, [[0.7652468076996379, 669, 25.73760676383972, {'criterion': 'mse', 'max_features': 'auto', 'n_estimators': 100}]])\n        func = <joblib.parallel.SafeFunction object>\n        args = ()\n        kwds = {}\n    120         except Exception as e:\n    121             if wrap_exception:\n    122                 e = ExceptionWithTraceback(e, e.__traceback__)\n    123             result = (False, e)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.SafeFunction object>, *args=(), **kwargs={})\n    125     def __init__(self, func):\n    126         self.func = func\n    127 \n    128     def __call__(self, *args, **kwargs):\n    129         try:\n--> 130             return self.func(*args, **kwargs)\n        self.func = <joblib.parallel.BatchedCalls object>\n        args = ()\n        kwargs = {}\n    131         except KeyboardInterrupt:\n    132             # We capture the KeyboardInterrupt and reraise it as\n    133             # something different, as multiprocessing does not\n    134             # interrupt processing for a KeyboardInterrupt\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {}), {'error_score': 'raise', 'return_parameters': True})]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/cross_validation.py in _fit_and_score(estimator=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, scorer=make_scorer(roc_auc_score, needs_threshold=True), train=array([  670,   671,   672, ..., 13386, 13387, 13388]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), verbose=0, parameters={'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1526 \n   1527     try:\n   1528         if y_train is None:\n   1529             estimator.fit(X_train, **fit_params)\n   1530         else:\n-> 1531             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestRegr...te=None,\n           verbose=0, warm_start=False)>\n        X_train =        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[12719 rows x 17 columns]\n        y_train = 6507     0\n12276    0\n4391     0\n18119    0\n9744...024    0\nName: label, Length: 12719, dtype: int64\n        fit_params = {}\n   1532 \n   1533     except Exception as e:\n   1534         if error_score == 'raise':\n   1535             raise\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py in fit(self=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=None)\n    285             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    286                              backend=\"threading\")(\n    287                 delayed(_parallel_build_trees)(\n    288                     t, self, X, y, sample_weight, i, len(trees),\n    289                     verbose=self.verbose, class_weight=self.class_weight)\n--> 290                 for i, t in enumerate(trees))\n        i = 99\n    291 \n    292             # Collect newly grown trees\n    293             self.estimators_.extend(trees)\n    294 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    805             if pre_dispatch == \"all\" or n_jobs == 1:\n    806                 # The iterable was consumed all at once by the above for loop.\n    807                 # No need to wait for async callbacks to trigger to\n    808                 # consumption.\n    809                 self._iterating = False\n--> 810             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    811             # Make sure that we get a last message telling us we are done\n    812             elapsed_time = time.time() - self._start_time\n    813             self._print('Done %3i out of %3i | elapsed: %s finished',\n    814                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nKeyError                                           Fri Nov  9 23:18:19 2018\nPID: 13242                                   Python 3.5.2: /usr/bin/python3\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _parallel_build_trees>, (DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), None, 0, 100), {'class_weight': None, 'verbose': 0})]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _parallel_build_trees>\n        args = (DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), None, 0, 100)\n        kwargs = {'class_weight': None, 'verbose': 0}\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py in _parallel_build_trees(tree=DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), forest=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=None, tree_idx=0, n_trees=100, verbose=0, class_weight=None)\n    111                 warnings.simplefilter('ignore', DeprecationWarning)\n    112                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n    113         elif class_weight == 'balanced_subsample':\n    114             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n    115 \n--> 116         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n        tree.fit = <bound method BaseDecisionTree.fit of DecisionTr...         random_state=19780174, splitter='best')>\n        X = array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32)\n        y = array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]])\n        sample_weight = None\n        curr_sample_weight = array([1., 1., 0., ..., 0., 2., 1.])\n    117     else:\n    118         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    119 \n    120     return tree\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/tree/tree.py in fit(self=DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=array([1., 1., 0., ..., 0., 2., 1.]), check_input=False, X_idx_sorted=None)\n    317         if not isinstance(criterion, Criterion):\n    318             if is_classification:\n    319                 criterion = CRITERIA_CLF[self.criterion](self.n_outputs_,\n    320                                                          self.n_classes_)\n    321             else:\n--> 322                 criterion = CRITERIA_REG[self.criterion](self.n_outputs_)\n        criterion = 'mae'\n        self.criterion = 'mae'\n        self.n_outputs_ = 1\n    323 \n    324         SPLITTERS = SPARSE_SPLITTERS if issparse(X) else DENSE_SPLITTERS\n    325 \n    326         splitter = self.splitter\n\nKeyError: 'mae'\n___________________________________________________________________________\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJoblibException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-462-88a493e9dcad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'max_features'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'criterion'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mae\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgrid_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandomforesttree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpre_dispatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgrid_drop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_drop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_drop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \"\"\"\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    551\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 for train, test in cv)\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    808\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0;31m# a working pool as they expect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibException\u001b[0m: JoblibException\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python3.5/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    179         sys.exit(msg)\n    180     main_globals = sys.modules[\"__main__\"].__dict__\n    181     if alter_argv:\n    182         sys.argv[0] = mod_spec.origin\n    183     return _run_code(code, main_globals, None,\n--> 184                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py')\n    185 \n    186 def run_module(mod_name, init_globals=None,\n    187                run_name=None, alter_sys=False):\n    188     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python3.5/runpy.py in _run_code(code=<code object <module> at 0x7f4e7a34cc90, file \"/...3.5/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/tito/.local/lib/python3.5/site-packages/__pycache__/ipykernel_launcher.cpython-35.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/tito/..../python3.5/site-packages/ipykernel/kernelapp.py'>, 'sys': <module 'sys' (built-in)>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f4e7a34cc90, file \"/...3.5/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/tito/.local/lib/python3.5/site-packages/__pycache__/ipykernel_launcher.cpython-35.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/tito/..../python3.5/site-packages/ipykernel/kernelapp.py'>, 'sys': <module 'sys' (built-in)>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n     17 \n     18 \n     19 \n     20 \n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    481         if self.poller is not None:\n    482             self.poller.start()\n    483         self.kernel.start()\n    484         self.io_loop = ioloop.IOLoop.current()\n    485         try:\n--> 486             self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    487         except KeyboardInterrupt:\n    488             pass\n    489 \n    490 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    107         except RuntimeError:\n    108             old_loop = None\n    109         try:\n    110             self._setup_logging()\n    111             asyncio.set_event_loop(self.asyncio_loop)\n--> 112             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Uni...EventLoop running=True closed=False debug=False>>\n    113         finally:\n    114             asyncio.set_event_loop(old_loop)\n    115 \n    116     def stop(self):\n\n...........................................................................\n/usr/lib/python3.5/asyncio/base_events.py in run_forever(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n    340             raise RuntimeError('Event loop is running.')\n    341         self._set_coroutine_wrapper(self._debug)\n    342         self._thread_id = threading.get_ident()\n    343         try:\n    344             while True:\n--> 345                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_UnixS...EventLoop running=True closed=False debug=False>>\n    346                 if self._stopping:\n    347                     break\n    348         finally:\n    349             self._stopping = False\n\n...........................................................................\n/usr/lib/python3.5/asyncio/base_events.py in _run_once(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n   1307                         logger.warning('Executing %s took %.3f seconds',\n   1308                                        _format_handle(handle), dt)\n   1309                 finally:\n   1310                     self._current_handle = None\n   1311             else:\n-> 1312                 handle._run()\n        handle._run = <bound method Handle._run of <Handle BaseAsyncIOLoop._handle_events(13, 1)>>\n   1313         handle = None  # Needed to break cycles when an exception occurs.\n   1314 \n   1315     def _set_coroutine_wrapper(self, enabled):\n   1316         try:\n\n...........................................................................\n/usr/lib/python3.5/asyncio/events.py in _run(self=<Handle BaseAsyncIOLoop._handle_events(13, 1)>)\n    120             self._callback = None\n    121             self._args = None\n    122 \n    123     def _run(self):\n    124         try:\n--> 125             self._callback(*self._args)\n        self._callback = <bound method BaseAsyncIOLoop._handle_events of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n        self._args = (13, 1)\n    126         except Exception as exc:\n    127             cb = _format_callback_source(self._callback, self._args)\n    128             msg = 'Exception in callback {}'.format(cb)\n    129             context = {\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py in _handle_events(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, fd=13, events=1)\n     97             self.writers.remove(fd)\n     98         del self.handlers[fd]\n     99 \n    100     def _handle_events(self, fd, events):\n    101         fileobj, handler_func = self.handlers[fd]\n--> 102         handler_func(fileobj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fileobj = <zmq.sugar.socket.Socket object>\n        events = 1\n    103 \n    104     def start(self):\n    105         try:\n    106             old_loop = asyncio.get_event_loop()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    445             return\n    446         zmq_events = self.socket.EVENTS\n    447         try:\n    448             # dispatch events:\n    449             if zmq_events & zmq.POLLIN and self.receiving():\n--> 450                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    451                 if not self.socket:\n    452                     return\n    453             if zmq_events & zmq.POLLOUT and self.sending():\n    454                 self._handle_send()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    475             else:\n    476                 raise\n    477         else:\n    478             if self._recv_callback:\n    479                 callback = self._recv_callback\n--> 480                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    481         \n    482 \n    483     def _handle_send(self):\n    484         \"\"\"Handle a send event.\"\"\"\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    427         close our socket.\"\"\"\n    428         try:\n    429             # Use a NullContext to ensure that all StackContexts are run\n    430             # inside our blanket exception handler rather than outside.\n    431             with stack_context.NullContext():\n--> 432                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    433         except:\n    434             gen_log.error(\"Uncaught exception in ZMQStream callback\",\n    435                           exc_info=True)\n    436             # Re-raise the exception so that IOLoop.handle_callback_exception\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'43d8b9a1122d4f57be7c23f396e940a0']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'43d8b9a1122d4f57be7c23f396e940a0'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>], cell_name='<ipython-input-462-88a493e9dcad>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>)\n   2845 \n   2846         try:\n   2847             for i, node in enumerate(to_run_exec):\n   2848                 mod = ast.Module([node])\n   2849                 code = compiler(mod, cell_name, \"exec\")\n-> 2850                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>\n        result = <ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>, result=<ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>\n        self.user_global_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...m sklearn.cross_validation import cross_val_score', 'features = pd.read_csv(\"features0,0.csv\")', 'randomforesttree = RandomForestClassifier(random...=103040, n_jobs=-1)\\nrandomforesttree.get_params()', 'features.columns', 'def setPred(col):\\n    if(col != 0):\\n        return 1\\n    else:\\n        return 0', '\\n\\nfeatures[\"prediction\"] = setPred(features[\"conversion\"])', 'features = pd.read_csv(\"features0,0.csv\")', 'setPred(features[\"conversion\"])', 'setPred(features[\"conversion\"].values)', 'features[\"conversion\"].apply(setPred)', 'features[\"conversion\"].apply(setPred).sum()', 'features[\"conversion\"]', 'features = pd.read_csv(\"features0,0.csv\")', 'features[\"prediction\"] = features[\"conversion\"]', 'features[\"prediction\"].mean()', 'features[\"prediction\"].max()', 'features[\"prediction\"] = features[\"conversion\"].applu(setPred)', 'features[\"prediction\"] = features[\"conversion\"].apply(setPred)', 'features[\"prediction\"].max()', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {3: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, 4: Index(['Unnamed: 0', 'ad campaign hit', 'brand l...   'nombre mas usado dia'],\n      dtype='object'), 10: 0       1\n1       1\n2       1\n3       1\n4       ...   1\nName: conversion, Length: 1712, dtype: int64, 11: 1712, 12: 0       2.0\n1       1.0\n2       1.0\n3       1.0\n....0\nName: conversion, Length: 1712, dtype: float64, 15: 0.1853565453785027, 16: 129.0, 19: 1, 21: 0.11221769134253451, 22: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, ...}\n        self.user_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...m sklearn.cross_validation import cross_val_score', 'features = pd.read_csv(\"features0,0.csv\")', 'randomforesttree = RandomForestClassifier(random...=103040, n_jobs=-1)\\nrandomforesttree.get_params()', 'features.columns', 'def setPred(col):\\n    if(col != 0):\\n        return 1\\n    else:\\n        return 0', '\\n\\nfeatures[\"prediction\"] = setPred(features[\"conversion\"])', 'features = pd.read_csv(\"features0,0.csv\")', 'setPred(features[\"conversion\"])', 'setPred(features[\"conversion\"].values)', 'features[\"conversion\"].apply(setPred)', 'features[\"conversion\"].apply(setPred).sum()', 'features[\"conversion\"]', 'features = pd.read_csv(\"features0,0.csv\")', 'features[\"prediction\"] = features[\"conversion\"]', 'features[\"prediction\"].mean()', 'features[\"prediction\"].max()', 'features[\"prediction\"] = features[\"conversion\"].applu(setPred)', 'features[\"prediction\"] = features[\"conversion\"].apply(setPred)', 'features[\"prediction\"].max()', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {3: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, 4: Index(['Unnamed: 0', 'ad campaign hit', 'brand l...   'nombre mas usado dia'],\n      dtype='object'), 10: 0       1\n1       1\n2       1\n3       1\n4       ...   1\nName: conversion, Length: 1712, dtype: int64, 11: 1712, 12: 0       2.0\n1       1.0\n2       1.0\n3       1.0\n....0\nName: conversion, Length: 1712, dtype: float64, 15: 0.1853565453785027, 16: 129.0, 19: 1, 21: 0.11221769134253451, 22: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\n/home/tito/Desktop/datas/gitNuevo/<ipython-input-462-88a493e9dcad> in <module>()\n      1 \n      2 \n----> 3 \n      4 param_grid= {'max_features': [\"auto\"], 'n_estimators': [100], 'criterion': ['mse',\"mae\"]}\n      5 grid_drop = GridSearchCV(randomforesttree, param_grid, cv=20, scoring='roc_auc',n_jobs = -1,pre_dispatch = 4)\n      6 grid_drop.fit(X_train, y_train)\n      7 print(grid_drop.best_params_, grid_drop.best_score_)\n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=20, error_score='raise',\n       ...atch=4, refit=True, scoring='roc_auc', verbose=0), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64)\n    799         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n    800             Target relative to X for classification or regression;\n    801             None for unsupervised learning.\n    802 \n    803         \"\"\"\n--> 804         return self._fit(X, y, ParameterGrid(self.param_grid))\n        self._fit = <bound method BaseSearchCV._fit of GridSearchCV(...tch=4, refit=True, scoring='roc_auc', verbose=0)>\n        X =        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns]\n        y = 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64\n        self.param_grid = {'criterion': ['mse', 'mae'], 'max_features': ['auto'], 'n_estimators': [100]}\n    805 \n    806 \n    807 class RandomizedSearchCV(BaseSearchCV):\n    808     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=20, error_score='raise',\n       ...atch=4, refit=True, scoring='roc_auc', verbose=0), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\n    548         )(\n    549             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    550                                     train, test, self.verbose, parameters,\n    551                                     self.fit_params, return_parameters=True,\n    552                                     error_score=self.error_score)\n--> 553                 for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterGrid object>\n    554                 for train, test in cv)\n    555 \n    556         # Out is a list of triplet: score, estimator, n_test_samples\n    557         n_fits = len(out)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV._fit.<locals>.<genexpr>>)\n    805             if pre_dispatch == \"all\" or n_jobs == 1:\n    806                 # The iterable was consumed all at once by the above for loop.\n    807                 # No need to wait for async callbacks to trigger to\n    808                 # consumption.\n    809                 self._iterating = False\n--> 810             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    811             # Make sure that we get a last message telling us we are done\n    812             elapsed_time = time.time() - self._start_time\n    813             self._print('Done %3i out of %3i | elapsed: %s finished',\n    814                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nJoblibKeyError                                     Fri Nov  9 23:18:21 2018\nPID: 13242                                   Python 3.5.2: /usr/bin/python3\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {}), {'error_score': 'raise', 'return_parameters': True})]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/cross_validation.py in _fit_and_score(estimator=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, scorer=make_scorer(roc_auc_score, needs_threshold=True), train=array([  670,   671,   672, ..., 13386, 13387, 13388]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), verbose=0, parameters={'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1526 \n   1527     try:\n   1528         if y_train is None:\n   1529             estimator.fit(X_train, **fit_params)\n   1530         else:\n-> 1531             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestRegr...te=None,\n           verbose=0, warm_start=False)>\n        X_train =        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[12719 rows x 17 columns]\n        y_train = 6507     0\n12276    0\n4391     0\n18119    0\n9744...024    0\nName: label, Length: 12719, dtype: int64\n        fit_params = {}\n   1532 \n   1533     except Exception as e:\n   1534         if error_score == 'raise':\n   1535             raise\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py in fit(self=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=None)\n    285             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    286                              backend=\"threading\")(\n    287                 delayed(_parallel_build_trees)(\n    288                     t, self, X, y, sample_weight, i, len(trees),\n    289                     verbose=self.verbose, class_weight=self.class_weight)\n--> 290                 for i, t in enumerate(trees))\n        i = 99\n    291 \n    292             # Collect newly grown trees\n    293             self.estimators_.extend(trees)\n    294 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    805             if pre_dispatch == \"all\" or n_jobs == 1:\n    806                 # The iterable was consumed all at once by the above for loop.\n    807                 # No need to wait for async callbacks to trigger to\n    808                 # consumption.\n    809                 self._iterating = False\n--> 810             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    811             # Make sure that we get a last message telling us we are done\n    812             elapsed_time = time.time() - self._start_time\n    813             self._print('Done %3i out of %3i | elapsed: %s finished',\n    814                         (len(self._output), len(self._output),\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in retrieve(self=Parallel(n_jobs=-1))\n    752                     # In case we had to terminate a managed pool, let\n    753                     # us start a new one to ensure that subsequent calls\n    754                     # to __call__ on the same Parallel instance will get\n    755                     # a working pool as they expect.\n    756                     self._initialize_pool()\n--> 757                 raise exception\n        exception = undefined\n    758 \n    759     def __call__(self, iterable):\n    760         if self._jobs:\n    761             raise ValueError('This Parallel instance is already running')\n\nJoblibKeyError: JoblibKeyError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python3.5/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    179         sys.exit(msg)\n    180     main_globals = sys.modules[\"__main__\"].__dict__\n    181     if alter_argv:\n    182         sys.argv[0] = mod_spec.origin\n    183     return _run_code(code, main_globals, None,\n--> 184                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py')\n    185 \n    186 def run_module(mod_name, init_globals=None,\n    187                run_name=None, alter_sys=False):\n    188     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python3.5/runpy.py in _run_code(code=<code object <module> at 0x7f4e7a34cc90, file \"/...3.5/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/tito/.local/lib/python3.5/site-packages/__pycache__/ipykernel_launcher.cpython-35.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/tito/..../python3.5/site-packages/ipykernel/kernelapp.py'>, 'sys': <module 'sys' (built-in)>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f4e7a34cc90, file \"/...3.5/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/tito/.local/lib/python3.5/site-packages/__pycache__/ipykernel_launcher.cpython-35.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.5/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/tito/..../python3.5/site-packages/ipykernel/kernelapp.py'>, 'sys': <module 'sys' (built-in)>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n     17 \n     18 \n     19 \n     20 \n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    481         if self.poller is not None:\n    482             self.poller.start()\n    483         self.kernel.start()\n    484         self.io_loop = ioloop.IOLoop.current()\n    485         try:\n--> 486             self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    487         except KeyboardInterrupt:\n    488             pass\n    489 \n    490 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    107         except RuntimeError:\n    108             old_loop = None\n    109         try:\n    110             self._setup_logging()\n    111             asyncio.set_event_loop(self.asyncio_loop)\n--> 112             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Uni...EventLoop running=True closed=False debug=False>>\n    113         finally:\n    114             asyncio.set_event_loop(old_loop)\n    115 \n    116     def stop(self):\n\n...........................................................................\n/usr/lib/python3.5/asyncio/base_events.py in run_forever(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n    340             raise RuntimeError('Event loop is running.')\n    341         self._set_coroutine_wrapper(self._debug)\n    342         self._thread_id = threading.get_ident()\n    343         try:\n    344             while True:\n--> 345                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_UnixS...EventLoop running=True closed=False debug=False>>\n    346                 if self._stopping:\n    347                     break\n    348         finally:\n    349             self._stopping = False\n\n...........................................................................\n/usr/lib/python3.5/asyncio/base_events.py in _run_once(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n   1307                         logger.warning('Executing %s took %.3f seconds',\n   1308                                        _format_handle(handle), dt)\n   1309                 finally:\n   1310                     self._current_handle = None\n   1311             else:\n-> 1312                 handle._run()\n        handle._run = <bound method Handle._run of <Handle BaseAsyncIOLoop._handle_events(13, 1)>>\n   1313         handle = None  # Needed to break cycles when an exception occurs.\n   1314 \n   1315     def _set_coroutine_wrapper(self, enabled):\n   1316         try:\n\n...........................................................................\n/usr/lib/python3.5/asyncio/events.py in _run(self=<Handle BaseAsyncIOLoop._handle_events(13, 1)>)\n    120             self._callback = None\n    121             self._args = None\n    122 \n    123     def _run(self):\n    124         try:\n--> 125             self._callback(*self._args)\n        self._callback = <bound method BaseAsyncIOLoop._handle_events of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n        self._args = (13, 1)\n    126         except Exception as exc:\n    127             cb = _format_callback_source(self._callback, self._args)\n    128             msg = 'Exception in callback {}'.format(cb)\n    129             context = {\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py in _handle_events(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, fd=13, events=1)\n     97             self.writers.remove(fd)\n     98         del self.handlers[fd]\n     99 \n    100     def _handle_events(self, fd, events):\n    101         fileobj, handler_func = self.handlers[fd]\n--> 102         handler_func(fileobj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fileobj = <zmq.sugar.socket.Socket object>\n        events = 1\n    103 \n    104     def start(self):\n    105         try:\n    106             old_loop = asyncio.get_event_loop()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    445             return\n    446         zmq_events = self.socket.EVENTS\n    447         try:\n    448             # dispatch events:\n    449             if zmq_events & zmq.POLLIN and self.receiving():\n--> 450                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    451                 if not self.socket:\n    452                     return\n    453             if zmq_events & zmq.POLLOUT and self.sending():\n    454                 self._handle_send()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    475             else:\n    476                 raise\n    477         else:\n    478             if self._recv_callback:\n    479                 callback = self._recv_callback\n--> 480                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    481         \n    482 \n    483     def _handle_send(self):\n    484         \"\"\"Handle a send event.\"\"\"\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    427         close our socket.\"\"\"\n    428         try:\n    429             # Use a NullContext to ensure that all StackContexts are run\n    430             # inside our blanket exception handler rather than outside.\n    431             with stack_context.NullContext():\n--> 432                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    433         except:\n    434             gen_log.error(\"Uncaught exception in ZMQStream callback\",\n    435                           exc_info=True)\n    436             # Re-raise the exception so that IOLoop.handle_callback_exception\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    271         # Fast path when there are no active contexts.\n    272         def null_wrapper(*args, **kwargs):\n    273             try:\n    274                 current_state = _state.contexts\n    275                 _state.contexts = cap_contexts[0]\n--> 276                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    277             finally:\n    278                 _state.contexts = current_state\n    279         null_wrapper._wrapped = True\n    280         return null_wrapper\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'43d8b9a1122d4f57be7c23f396e940a0']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'43d8b9a1122d4f57be7c23f396e940a0'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 11, 10, 2, 15, 54, 365781, tzinfo=datetime.timezone.utc), 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'session': '43d8b9a1122d4f57be7c23f396e940a0', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '337a330962454de2b80f95702274a564', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='param_grid= {\\'max_features\\': [\"auto\"], \\'n_estima...nt(grid_drop.best_params_, grid_drop.best_score_)', store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>], cell_name='<ipython-input-462-88a493e9dcad>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>)\n   2845 \n   2846         try:\n   2847             for i, node in enumerate(to_run_exec):\n   2848                 mod = ast.Module([node])\n   2849                 code = compiler(mod, cell_name, \"exec\")\n-> 2850                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>\n        result = <ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n\n...........................................................................\n/home/tito/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>, result=<ExecutionResult object at 7f4e35348c50, executi..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f4e3553c660, file \"<ipython-input-462-88a493e9dcad>\", line 3>\n        self.user_global_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...m sklearn.cross_validation import cross_val_score', 'features = pd.read_csv(\"features0,0.csv\")', 'randomforesttree = RandomForestClassifier(random...=103040, n_jobs=-1)\\nrandomforesttree.get_params()', 'features.columns', 'def setPred(col):\\n    if(col != 0):\\n        return 1\\n    else:\\n        return 0', '\\n\\nfeatures[\"prediction\"] = setPred(features[\"conversion\"])', 'features = pd.read_csv(\"features0,0.csv\")', 'setPred(features[\"conversion\"])', 'setPred(features[\"conversion\"].values)', 'features[\"conversion\"].apply(setPred)', 'features[\"conversion\"].apply(setPred).sum()', 'features[\"conversion\"]', 'features = pd.read_csv(\"features0,0.csv\")', 'features[\"prediction\"] = features[\"conversion\"]', 'features[\"prediction\"].mean()', 'features[\"prediction\"].max()', 'features[\"prediction\"] = features[\"conversion\"].applu(setPred)', 'features[\"prediction\"] = features[\"conversion\"].apply(setPred)', 'features[\"prediction\"].max()', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {3: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, 4: Index(['Unnamed: 0', 'ad campaign hit', 'brand l...   'nombre mas usado dia'],\n      dtype='object'), 10: 0       1\n1       1\n2       1\n3       1\n4       ...   1\nName: conversion, Length: 1712, dtype: int64, 11: 1712, 12: 0       2.0\n1       1.0\n2       1.0\n3       1.0\n....0\nName: conversion, Length: 1712, dtype: float64, 15: 0.1853565453785027, 16: 129.0, 19: 1, 21: 0.11221769134253451, 22: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, ...}\n        self.user_ns = {'AdaBoostRegressor': <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, 'BaggingRegressor': <class 'sklearn.ensemble.bagging.BaggingRegressor'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'DecisionTreeRegressor': <class 'sklearn.tree.tree.DecisionTreeRegressor'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...m sklearn.cross_validation import cross_val_score', 'features = pd.read_csv(\"features0,0.csv\")', 'randomforesttree = RandomForestClassifier(random...=103040, n_jobs=-1)\\nrandomforesttree.get_params()', 'features.columns', 'def setPred(col):\\n    if(col != 0):\\n        return 1\\n    else:\\n        return 0', '\\n\\nfeatures[\"prediction\"] = setPred(features[\"conversion\"])', 'features = pd.read_csv(\"features0,0.csv\")', 'setPred(features[\"conversion\"])', 'setPred(features[\"conversion\"].values)', 'features[\"conversion\"].apply(setPred)', 'features[\"conversion\"].apply(setPred).sum()', 'features[\"conversion\"]', 'features = pd.read_csv(\"features0,0.csv\")', 'features[\"prediction\"] = features[\"conversion\"]', 'features[\"prediction\"].mean()', 'features[\"prediction\"].max()', 'features[\"prediction\"] = features[\"conversion\"].applu(setPred)', 'features[\"prediction\"] = features[\"conversion\"].apply(setPred)', 'features[\"prediction\"].max()', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {3: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, 4: Index(['Unnamed: 0', 'ad campaign hit', 'brand l...   'nombre mas usado dia'],\n      dtype='object'), 10: 0       1\n1       1\n2       1\n3       1\n4       ...   1\nName: conversion, Length: 1712, dtype: int64, 11: 1712, 12: 0       2.0\n1       1.0\n2       1.0\n3       1.0\n....0\nName: conversion, Length: 1712, dtype: float64, 15: 0.1853565453785027, 16: 129.0, 19: 1, 21: 0.11221769134253451, 22: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, ...}, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\n/home/tito/Desktop/datas/gitNuevo/<ipython-input-462-88a493e9dcad> in <module>()\n      1 \n      2 \n----> 3 \n      4 param_grid= {'max_features': [\"auto\"], 'n_estimators': [100], 'criterion': ['mse',\"mae\"]}\n      5 grid_drop = GridSearchCV(randomforesttree, param_grid, cv=20, scoring='roc_auc',n_jobs = -1,pre_dispatch = 4)\n      6 grid_drop.fit(X_train, y_train)\n      7 print(grid_drop.best_params_, grid_drop.best_score_)\n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=20, error_score='raise',\n       ...atch=4, refit=True, scoring='roc_auc', verbose=0), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64)\n    799         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n    800             Target relative to X for classification or regression;\n    801             None for unsupervised learning.\n    802 \n    803         \"\"\"\n--> 804         return self._fit(X, y, ParameterGrid(self.param_grid))\n        self._fit = <bound method BaseSearchCV._fit of GridSearchCV(...tch=4, refit=True, scoring='roc_auc', verbose=0)>\n        X =        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns]\n        y = 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64\n        self.param_grid = {'criterion': ['mse', 'mae'], 'max_features': ['auto'], 'n_estimators': [100]}\n    805 \n    806 \n    807 class RandomizedSearchCV(BaseSearchCV):\n    808     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=20, error_score='raise',\n       ...atch=4, refit=True, scoring='roc_auc', verbose=0), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\n    548         )(\n    549             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    550                                     train, test, self.verbose, parameters,\n    551                                     self.fit_params, return_parameters=True,\n    552                                     error_score=self.error_score)\n--> 553                 for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterGrid object>\n    554                 for train, test in cv)\n    555 \n    556         # Out is a list of triplet: score, estimator, n_test_samples\n    557         n_fits = len(out)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV._fit.<locals>.<genexpr>>)\n    761             raise ValueError('This Parallel instance is already running')\n    762         # A flag used to abort the dispatching of jobs in case an\n    763         # exception is found\n    764         self._aborting = False\n    765         if not self._managed_pool:\n--> 766             n_jobs = self._initialize_pool()\n        n_jobs = undefined\n        self._initialize_pool = <bound method Parallel._initialize_pool of Parallel(n_jobs=-1)>\n    767         else:\n    768             n_jobs = self._effective_n_jobs()\n    769 \n    770         if self.batch_size == 'auto':\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in _initialize_pool(self=Parallel(n_jobs=-1))\n    532                     verbose=max(0, self.verbose - 50),\n    533                 )\n    534                 if self._mp_context is not None:\n    535                     # Use Python 3.4+ multiprocessing context isolation\n    536                     poolargs['context'] = self._mp_context\n--> 537                 self._pool = MemmapingPool(n_jobs, **poolargs)\n        self._pool = None\n        n_jobs = 4\n        poolargs = {'context': <multiprocessing.context.ForkContext object>, 'max_nbytes': 1048576, 'mmap_mode': 'r', 'temp_folder': None, 'verbose': 0}\n    538 \n    539                 # We are using multiprocessing, we also want to capture\n    540                 # KeyboardInterrupts\n    541                 self.exceptions.extend([KeyboardInterrupt, WorkerInterrupt])\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/pool.py in __init__(self=<joblib.pool.MemmapingPool object>, processes=4, temp_folder='/dev/shm', max_nbytes=1048576, mmap_mode='r', forward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, backward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, verbose=0, context_id=None, prewarm=False, **kwargs={'context': <multiprocessing.context.ForkContext object>})\n    575         poolargs = dict(\n    576             processes=processes,\n    577             forward_reducers=forward_reducers,\n    578             backward_reducers=backward_reducers)\n    579         poolargs.update(kwargs)\n--> 580         super(MemmapingPool, self).__init__(**poolargs)\n        self.__init__ = <bound method MemmapingPool.__init__ of <joblib.pool.MemmapingPool object>>\n        poolargs = {'backward_reducers': {<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, 'context': <multiprocessing.context.ForkContext object>, 'forward_reducers': {<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, 'processes': 4}\n    581 \n    582     def terminate(self):\n    583         super(MemmapingPool, self).terminate()\n    584         delete_folder(self._temp_folder)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/pool.py in __init__(self=<joblib.pool.MemmapingPool object>, processes=4, forward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, backward_reducers={<class 'numpy.ndarray'>: <joblib.pool.ArrayMemmapReducer object>, <class 'numpy.core.memmap.memmap'>: <function reduce_memmap>}, **kwargs={'context': <multiprocessing.context.ForkContext object>})\n    413             backward_reducers = dict()\n    414         self._forward_reducers = forward_reducers\n    415         self._backward_reducers = backward_reducers\n    416         poolargs = dict(processes=processes)\n    417         poolargs.update(kwargs)\n--> 418         super(PicklingPool, self).__init__(**poolargs)\n        self.__init__ = <bound method MemmapingPool.__init__ of <joblib.pool.MemmapingPool object>>\n        poolargs = {'context': <multiprocessing.context.ForkContext object>, 'processes': 4}\n    419 \n    420     def _setup_queues(self):\n    421         context = getattr(self, '_ctx', mp)\n    422         self._inqueue = CustomizablePicklingQueue(context,\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/pool.py in __init__(self=<joblib.pool.MemmapingPool object>, processes=4, initializer=None, initargs=(), maxtasksperchild=None, context=<multiprocessing.context.ForkContext object>)\n    163         if initializer is not None and not callable(initializer):\n    164             raise TypeError('initializer must be a callable')\n    165 \n    166         self._processes = processes\n    167         self._pool = []\n--> 168         self._repopulate_pool()\n        self._repopulate_pool = <bound method Pool._repopulate_pool of <joblib.pool.MemmapingPool object>>\n    169 \n    170         self._worker_handler = threading.Thread(\n    171             target=Pool._handle_workers,\n    172             args=(self, )\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/pool.py in _repopulate_pool(self=<joblib.pool.MemmapingPool object>)\n    228                                    self._wrap_exception)\n    229                             )\n    230             self._pool.append(w)\n    231             w.name = w.name.replace('Process', 'PoolWorker')\n    232             w.daemon = True\n--> 233             w.start()\n        w.start = <bound method BaseProcess.start of <ForkProcess(ForkPoolWorker-130, started daemon)>>\n    234             util.debug('added worker')\n    235 \n    236     def _maintain_pool(self):\n    237         \"\"\"Clean up any exited workers and start replacements for them.\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/process.py in start(self=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n    100         assert self._parent_pid == os.getpid(), \\\n    101                'can only start a process object created by current process'\n    102         assert not _current_process._config.get('daemon'), \\\n    103                'daemonic processes are not allowed to have children'\n    104         _cleanup()\n--> 105         self._popen = self._Popen(self)\n        self._popen = None\n        self._Popen = <function ForkProcess._Popen>\n        self = <ForkProcess(ForkPoolWorker-130, started daemon)>\n    106         self._sentinel = self._popen.sentinel\n    107         _children.add(self)\n    108 \n    109     def terminate(self):\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/context.py in _Popen(process_obj=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n    262     class ForkProcess(process.BaseProcess):\n    263         _start_method = 'fork'\n    264         @staticmethod\n    265         def _Popen(process_obj):\n    266             from .popen_fork import Popen\n--> 267             return Popen(process_obj)\n        Popen = <class 'multiprocessing.popen_fork.Popen'>\n        process_obj = <ForkProcess(ForkPoolWorker-130, started daemon)>\n    268 \n    269     class SpawnProcess(process.BaseProcess):\n    270         _start_method = 'spawn'\n    271         @staticmethod\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/popen_fork.py in __init__(self=<multiprocessing.popen_fork.Popen object>, process_obj=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n     15 \n     16     def __init__(self, process_obj):\n     17         sys.stdout.flush()\n     18         sys.stderr.flush()\n     19         self.returncode = None\n---> 20         self._launch(process_obj)\n        self._launch = <bound method Popen._launch of <multiprocessing.popen_fork.Popen object>>\n        process_obj = <ForkProcess(ForkPoolWorker-130, started daemon)>\n     21 \n     22     def duplicate_for_child(self, fd):\n     23         return fd\n     24 \n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/popen_fork.py in _launch(self=<multiprocessing.popen_fork.Popen object>, process_obj=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n     69             try:\n     70                 os.close(parent_r)\n     71                 if 'random' in sys.modules:\n     72                     import random\n     73                     random.seed()\n---> 74                 code = process_obj._bootstrap()\n        code = 1\n        process_obj._bootstrap = <bound method BaseProcess._bootstrap of <ForkProcess(ForkPoolWorker-130, started daemon)>>\n     75             finally:\n     76                 os._exit(code)\n     77         else:\n     78             os.close(child_w)\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/process.py in _bootstrap(self=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n    244                 # delay finalization of the old process object until after\n    245                 # _run_after_forkers() is executed\n    246                 del old_process\n    247             util.info('child process calling self.run()')\n    248             try:\n--> 249                 self.run()\n        self.run = <bound method BaseProcess.run of <ForkProcess(ForkPoolWorker-130, started daemon)>>\n    250                 exitcode = 0\n    251             finally:\n    252                 util._exit_function()\n    253         except SystemExit as e:\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/process.py in run(self=<ForkProcess(ForkPoolWorker-130, started daemon)>)\n     88     def run(self):\n     89         '''\n     90         Method to be run in sub-process; can be overridden in sub-class\n     91         '''\n     92         if self._target:\n---> 93             self._target(*self._args, **self._kwargs)\n        self._target = <function worker>\n        self._args = (<joblib.pool.CustomizablePicklingQueue object>, <joblib.pool.CustomizablePicklingQueue object>, None, (), None, True)\n        self._kwargs = {}\n     94 \n     95     def start(self):\n     96         '''\n     97         Start child process\n\n...........................................................................\n/usr/lib/python3.5/multiprocessing/pool.py in worker(inqueue=<joblib.pool.CustomizablePicklingQueue object>, outqueue=<joblib.pool.CustomizablePicklingQueue object>, initializer=None, initargs=(), maxtasks=None, wrap_exception=True)\n    114             util.debug('worker got sentinel -- exiting')\n    115             break\n    116 \n    117         job, i, func, args, kwds = task\n    118         try:\n--> 119             result = (True, func(*args, **kwds))\n        result = (True, [[0.7652468076996379, 669, 25.73760676383972, {'criterion': 'mse', 'max_features': 'auto', 'n_estimators': 100}]])\n        func = <joblib.parallel.SafeFunction object>\n        args = ()\n        kwds = {}\n    120         except Exception as e:\n    121             if wrap_exception:\n    122                 e = ExceptionWithTraceback(e, e.__traceback__)\n    123             result = (False, e)\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.SafeFunction object>, *args=(), **kwargs={})\n    125     def __init__(self, func):\n    126         self.func = func\n    127 \n    128     def __call__(self, *args, **kwargs):\n    129         try:\n--> 130             return self.func(*args, **kwargs)\n        self.func = <joblib.parallel.BatchedCalls object>\n        args = ()\n        kwargs = {}\n    131         except KeyboardInterrupt:\n    132             # We capture the KeyboardInterrupt and reraise it as\n    133             # something different, as multiprocessing does not\n    134             # interrupt processing for a KeyboardInterrupt\n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {}), {'error_score': 'raise', 'return_parameters': True})]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False),        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], 17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([  670,   671,   672, ..., 13386, 13387, 13388]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), 0, {'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, {})\n        kwargs = {'error_score': 'raise', 'return_parameters': True}\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/cross_validation.py in _fit_and_score(estimator=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=       ad campaign hit  brand listing  checkout ...            0.121452  \n\n[13389 rows x 17 columns], y=17919    0\n17858    0\n8744     0\n4177     0\n1788...024    0\nName: label, Length: 13389, dtype: int64, scorer=make_scorer(roc_auc_score, needs_threshold=True), train=array([  670,   671,   672, ..., 13386, 13387, 13388]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ..., 662,\n       663, 664, 665, 666, 667, 668, 669]), verbose=0, parameters={'criterion': 'mae', 'max_features': 'auto', 'n_estimators': 100}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1526 \n   1527     try:\n   1528         if y_train is None:\n   1529             estimator.fit(X_train, **fit_params)\n   1530         else:\n-> 1531             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestRegr...te=None,\n           verbose=0, warm_start=False)>\n        X_train =        ad campaign hit  brand listing  checkout ...            0.121452  \n\n[12719 rows x 17 columns]\n        y_train = 6507     0\n12276    0\n4391     0\n18119    0\n9744...024    0\nName: label, Length: 12719, dtype: int64\n        fit_params = {}\n   1532 \n   1533     except Exception as e:\n   1534         if error_score == 'raise':\n   1535             raise\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py in fit(self=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=None)\n    285             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    286                              backend=\"threading\")(\n    287                 delayed(_parallel_build_trees)(\n    288                     t, self, X, y, sample_weight, i, len(trees),\n    289                     verbose=self.verbose, class_weight=self.class_weight)\n--> 290                 for i, t in enumerate(trees))\n        i = 99\n    291 \n    292             # Collect newly grown trees\n    293             self.estimators_.extend(trees)\n    294 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    805             if pre_dispatch == \"all\" or n_jobs == 1:\n    806                 # The iterable was consumed all at once by the above for loop.\n    807                 # No need to wait for async callbacks to trigger to\n    808                 # consumption.\n    809                 self._iterating = False\n--> 810             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    811             # Make sure that we get a last message telling us we are done\n    812             elapsed_time = time.time() - self._start_time\n    813             self._print('Done %3i out of %3i | elapsed: %s finished',\n    814                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nKeyError                                           Fri Nov  9 23:18:19 2018\nPID: 13242                                   Python 3.5.2: /usr/bin/python3\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _parallel_build_trees>, (DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), None, 0, 100), {'class_weight': None, 'verbose': 0})]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _parallel_build_trees>\n        args = (DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), None, 0, 100)\n        kwargs = {'class_weight': None, 'verbose': 0}\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/ensemble/forest.py in _parallel_build_trees(tree=DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), forest=RandomForestRegressor(bootstrap=True, criterion=...ate=None,\n           verbose=0, warm_start=False), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=None, tree_idx=0, n_trees=100, verbose=0, class_weight=None)\n    111                 warnings.simplefilter('ignore', DeprecationWarning)\n    112                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n    113         elif class_weight == 'balanced_subsample':\n    114             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n    115 \n--> 116         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n        tree.fit = <bound method BaseDecisionTree.fit of DecisionTr...         random_state=19780174, splitter='best')>\n        X = array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32)\n        y = array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]])\n        sample_weight = None\n        curr_sample_weight = array([1., 1., 0., ..., 0., 2., 1.])\n    117     else:\n    118         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    119 \n    120     return tree\n\n...........................................................................\n/usr/lib/python3/dist-packages/sklearn/tree/tree.py in fit(self=DecisionTreeRegressor(criterion='mae', max_depth...          random_state=19780174, splitter='best'), X=array([[ 0.        ,  4.        ,  1.        , ....        0.1308905 ,  0.12145183]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]]), sample_weight=array([1., 1., 0., ..., 0., 2., 1.]), check_input=False, X_idx_sorted=None)\n    317         if not isinstance(criterion, Criterion):\n    318             if is_classification:\n    319                 criterion = CRITERIA_CLF[self.criterion](self.n_outputs_,\n    320                                                          self.n_classes_)\n    321             else:\n--> 322                 criterion = CRITERIA_REG[self.criterion](self.n_outputs_)\n        criterion = 'mae'\n        self.criterion = 'mae'\n        self.n_outputs_ = 1\n    323 \n    324         SPLITTERS = SPARSE_SPLITTERS if issparse(X) else DENSE_SPLITTERS\n    325 \n    326         splitter = self.splitter\n\nKeyError: 'mae'\n___________________________________________________________________________\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "param_grid= {'max_features': [\"auto\"], 'n_estimators': [100], 'criterion': ['mse']}\n",
    "grid_drop = GridSearchCV(randomforesttree, param_grid, cv=20, scoring='roc_auc',n_jobs = -1,pre_dispatch = 4)\n",
    "grid_drop.fit(X_train, y_train)\n",
    "print(grid_drop.best_params_, grid_drop.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = grid_drop.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(grid_drop.best_estimator_.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>condition_no_convercion2</th>\n",
       "      <td>0.124678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_no_convercion2</th>\n",
       "      <td>0.115660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nombre mas usado dia2</th>\n",
       "      <td>0.110516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_engine2</th>\n",
       "      <td>0.104809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>checkout</th>\n",
       "      <td>0.082547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>viewed product</th>\n",
       "      <td>0.070564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numero mas usado dia</th>\n",
       "      <td>0.067453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand listing</th>\n",
       "      <td>0.055407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visited site</th>\n",
       "      <td>0.048127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generic listing</th>\n",
       "      <td>0.044892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ad campaign hit</th>\n",
       "      <td>0.043466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>searched products</th>\n",
       "      <td>0.039537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search engine hit</th>\n",
       "      <td>0.030575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storage_no_convercion</th>\n",
       "      <td>0.030490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversion</th>\n",
       "      <td>0.014321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>staticpage</th>\n",
       "      <td>0.012177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead</th>\n",
       "      <td>0.004780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          importance\n",
       "condition_no_convercion2    0.124678\n",
       "color_no_convercion2        0.115660\n",
       "nombre mas usado dia2       0.110516\n",
       "search_engine2              0.104809\n",
       "checkout                    0.082547\n",
       "viewed product              0.070564\n",
       "numero mas usado dia        0.067453\n",
       "brand listing               0.055407\n",
       "visited site                0.048127\n",
       "generic listing             0.044892\n",
       "ad campaign hit             0.043466\n",
       "searched products           0.039537\n",
       "search engine hit           0.030575\n",
       "storage_no_convercion       0.030490\n",
       "conversion                  0.014321\n",
       "staticpage                  0.012177\n",
       "lead                        0.004780"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost\n",
    "# historial cambios tweaks | score \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'deviance',\n",
       " 'max_depth': 3,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'presort': 'auto',\n",
       " 'random_state': None,\n",
       " 'subsample': 1.0,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientboost = GradientBoostingClassifier(random_state = None)\n",
    "gradientboost.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'auto', 'loss': 'exponential', 'n_estimators': 30} 0.8274479183851948\n"
     ]
    }
   ],
   "source": [
    "param_grid= {'max_features': [\"auto\"], 'n_estimators': [30], 'loss': ['deviance', 'exponential']}\n",
    "grid_drop = GridSearchCV(gradientboost, param_grid, cv=20, scoring='roc_auc')\n",
    "grid_drop.fit(X_train, y_train)\n",
    "print(grid_drop.best_params_, grid_drop.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = grid_drop.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>checkout</th>\n",
       "      <td>0.138690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_no_convercion2</th>\n",
       "      <td>0.095729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_engine2</th>\n",
       "      <td>0.095588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition_no_convercion2</th>\n",
       "      <td>0.092753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nombre mas usado dia2</th>\n",
       "      <td>0.088604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>viewed product</th>\n",
       "      <td>0.079509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numero mas usado dia</th>\n",
       "      <td>0.073116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visited site</th>\n",
       "      <td>0.052279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand listing</th>\n",
       "      <td>0.051483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generic listing</th>\n",
       "      <td>0.051276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ad campaign hit</th>\n",
       "      <td>0.047105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search engine hit</th>\n",
       "      <td>0.036473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>searched products</th>\n",
       "      <td>0.035282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storage_no_convercion</th>\n",
       "      <td>0.030384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversion</th>\n",
       "      <td>0.018615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>staticpage</th>\n",
       "      <td>0.009851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead</th>\n",
       "      <td>0.003263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          importance\n",
       "checkout                    0.138690\n",
       "color_no_convercion2        0.095729\n",
       "search_engine2              0.095588\n",
       "condition_no_convercion2    0.092753\n",
       "nombre mas usado dia2       0.088604\n",
       "viewed product              0.079509\n",
       "numero mas usado dia        0.073116\n",
       "visited site                0.052279\n",
       "brand listing               0.051483\n",
       "generic listing             0.051276\n",
       "ad campaign hit             0.047105\n",
       "search engine hit           0.036473\n",
       "searched products           0.035282\n",
       "storage_no_convercion       0.030384\n",
       "conversion                  0.018615\n",
       "staticpage                  0.009851\n",
       "lead                        0.003263"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances2 = pd.DataFrame(grid_drop.best_estimator_.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pred = pd.read_csv(\"csv/trocafone_kaggle_test.csv\")\n",
    "features = pd.read_csv(\"features0,0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.drop([\"Unnamed: 0\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.drop([\"level_0\"],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pred_completo = pd.merge(to_pred,features,on=\"person\",how = \"left\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pred_completo[\"label\"] = grid_drop.best_estimator_.predict(to_pred_completo.drop([\"person\"],axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pred = pd.merge(to_pred_completo[[\"person\",\"label\"]],to_pred,on=\"person\",how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pred = to_pred.fillna(to_pred[\"label\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pred.to_csv(\"sumit0.0\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
